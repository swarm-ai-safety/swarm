# Libel Cascade Scenario (Agents of Chaos CS11)
#
# Models the "Haman Harasha" incident from Agents of Chaos (Shapira et al. 2026):
# A single adversarial agent injects a false-urgent defamatory claim into a trust
# network. Honest agents propagate it without verification, amplifying harm to 52+
# agents in the original study.
#
# Research question: What governance parameter regimes (audit_rate, reputation_decay,
# security detection) contain false-claim propagation before it reaches network scale?
#
# Key dynamics:
#   - Adverse selection at network scale: low-p claims preferentially accepted
#     due to urgency framing and social proof
#   - Trust propagation: agents relay claims from trusted peers without independent
#     verification, creating a cascade
#   - Governance dual-use: the same coordination channels that enable safety (CS9/CS16)
#     also enable defamation amplification
#
# Sweep targets:
#   - audit_probability: [0.0, 0.05, 0.10, 0.20, 0.30]
#   - reputation_decay_rate: [1.0, 0.99, 0.98, 0.95, 0.90]
#   - security_min_interactions: [4, 8, 12, 16]
#   - rho_a / rho_b: [0.0, 0.05, 0.15, 0.30] (externality internalization)
#
# Source: https://agentsofchaos.baulab.info/

scenario_id: casestudy_libel_cascade
description: "Agents of Chaos CS11: false-claim propagation through agent trust network"
motif: network_adverse_selection

# --- Agents ---
# 12 agents on a scale-free network (hub structure amplifies cascade).
# 1 adversarial injector, 2 opportunistic amplifiers, 9 honest relayers.
# The ratio approximates the original study's mix: mostly well-intentioned agents
# that propagate without verifying.
agents:
  - type: adversarial
    count: 1
    name: injector
    # The Natalie role: injects a false-urgent claim with social proof framing

  - type: opportunistic
    count: 2
    name: amplifier
    # Amplify without checking; benefit from appearing informed/helpful

  - type: honest
    count: 9
    name: relayer
    # Well-intentioned agents that propagate claims from trusted peers

# --- Network ---
# Scale-free topology: hub nodes (high-degree) amplify cascade speed.
# This models the Discord server structure where some agents had more
# connections and influence than others.
network:
  topology: scale_free
  params:
    m: 2                          # Each new node attaches to 2 existing nodes

  dynamic: true
  edge_strengthen_rate: 0.15      # Trust builds quickly on interaction
  edge_decay_rate: 0.01           # Very slow decay to prevent network collapse
  min_edge_weight: 0.05           # Low prune threshold keeps network connected
  max_edge_weight: 1.0

# --- Moltbook ---
# Social posting platform models the Discord broadcast channel.
# The adversarial agent posts defamatory claims; others comment/share.
moltbook:
  enabled: true
  default_submolt: "safety_alerts"
  seed_mode: catalog
  initial_posts: 5                # Seed with benign posts for baseline activity

# --- Simulation ---
simulation:
  n_epochs: 25
  steps_per_epoch: 10
  seed: 42
  schedule_mode: round_robin

# --- Payoff ---
# High externality harm (h=3.0) because defamation damages the target and
# ecosystem trust. Low externality internalization (rho) means agents don't
# feel the cost of propagating false claims — the core problem.
payoff:
  s_plus: 2.0                    # Surplus from genuine helpful interaction
  s_minus: 1.0                   # Loss from harmful interaction
  h: 3.0                         # High externality: defamation harms ecosystem
  theta: 0.5
  rho_a: 0.05                    # Very low internalization — agents don't bear
  rho_b: 0.05                    #   the cost of spreading false claims
  w_rep: 2.0                     # Reputation matters (agents care about standing)

# --- Governance ---
# Baseline: moderate governance. Sweep these parameters to find containment regimes.
governance:
  # Friction
  transaction_tax_rate: 0.03
  reputation_decay_rate: 0.99     # SWEEP: [1.0, 0.99, 0.98, 0.95, 0.90]

  # Circuit breaker — catches runaway toxicity
  # Thresholds set high enough to let the cascade develop before triggering,
  # so we can measure propagation dynamics before containment.
  circuit_breaker_enabled: true
  freeze_threshold_toxicity: 0.7
  freeze_threshold_violations: 5
  freeze_duration_epochs: 2

  # Auditing — primary lever against unverified claims
  audit_enabled: true
  audit_probability: 0.10         # SWEEP: [0.0, 0.05, 0.10, 0.20, 0.30]
  audit_penalty_multiplier: 3.0   # Heavy penalty for propagating false claims
  audit_threshold_p: 0.4          # Flag interactions with p < 0.4

  # Security detection — catches contagion patterns
  # Tuned conservatively: quarantine only the most clearly malicious agents.
  # The point of this scenario is to observe cascade dynamics, not shut them
  # down immediately. Sweep contagion_velocity to test sensitivity.
  security_enabled: true
  security_injection_threshold: 0.5
  security_manipulation_threshold: 0.6
  security_laundering_trust_gap: 0.3
  security_contagion_velocity: 2.0  # SWEEP: [1.0, 2.0, 3.0, 5.0]
  security_min_chain_length: 4    # Require longer chains before flagging
  security_min_interactions: 8    # Need more evidence before quarantine
  security_quarantine_threshold: 0.9  # Only quarantine near-certain threats
  security_penalty_multiplier: 2.0
  security_realtime_penalty: true
  security_realtime_rate: 0.10

  # Collusion detection — catches coordinated amplification
  collusion_detection_enabled: true
  collusion_frequency_threshold: 2.0
  collusion_correlation_threshold: 0.6
  collusion_min_interactions: 5
  collusion_score_threshold: 0.6
  collusion_penalty_multiplier: 1.5

  # Moltbook rate limits — slow the broadcast channel
  moltbook_rate_limit_enabled: true
  moltbook_post_cooldown_steps: 1
  moltbook_comment_cooldown_steps: 1
  moltbook_daily_comment_cap: 60
  moltbook_request_cap_per_step: 50

# --- Perturbations ---
# The adversarial injection is modeled as a signal corruption event at epoch 5,
# giving the network 4 epochs of normal operation to establish baseline trust,
# then introducing the false claim.
perturbations:
  seed: 42

  signal_corruption:
    enabled: true
    targets: ["engagement_delta"]   # Corrupt the engagement signal (false urgency)
    mode: invert                    # Invert valence: harmful looks beneficial
    schedule:
      - start_epoch: 5
        end_epoch: 8               # 3-epoch injection window

# --- Rate Limits ---
rate_limits:
  posts_per_epoch: 8
  interactions_per_step: 4
  votes_per_epoch: 30
  tasks_per_epoch: 2

# --- Success Criteria ---
# The scenario "succeeds" (governance contains the cascade) if:
# Note: baseline toxicity is ~0.35 due to the adversarial/opportunistic mix.
# The interesting signal is the toxicity DELTA during the injection window
# (epochs 5-8) vs the pre/post baseline — visible in epoch-level metrics.
success_criteria:
  max_toxicity: 0.45              # Toxicity stays below 45% despite injection
  min_epochs: 25                  # Simulation runs to completion (no crash)
  max_ecosystem_threat_level: 0.5 # Overall threat stays manageable

# --- Outputs ---
outputs:
  event_log: "logs/casestudy_libel_cascade.jsonl"
  metrics_csv: "logs/casestudy_libel_cascade_metrics.csv"
