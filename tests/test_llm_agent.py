"""Tests for LLM-backed agents."""

from unittest.mock import AsyncMock, MagicMock, patch

import pytest

from swarm.agents.base import ActionType, Observation
from swarm.agents.llm_config import (
    LLMConfig,
    LLMProvider,
    LLMUsageStats,
    PersonaType,
)
from swarm.agents.llm_prompts import (
    PERSONA_PROMPTS,
    build_accept_prompt,
    build_action_prompt,
    format_observation,
)
from swarm.models.agent import AgentState, AgentType

# =============================================================================
# Fixtures
# =============================================================================


@pytest.fixture
def basic_llm_config():
    """Basic LLM configuration for testing."""
    return LLMConfig(
        provider=LLMProvider.ANTHROPIC,
        model="claude-sonnet-4-20250514",
        api_key="test-key",
        temperature=0.7,
        max_tokens=512,
    )


@pytest.fixture
def basic_observation():
    """Basic observation for testing."""
    return Observation(
        agent_state=AgentState(
            agent_id="test_agent",
            agent_type=AgentType.HONEST,
            reputation=5.0,
            resources=100.0,
        ),
        current_epoch=1,
        current_step=5,
        visible_posts=[
            {
                "post_id": "post_1",
                "author_id": "other_agent",
                "content": "Hello world",
                "score": 10,
            },
        ],
        visible_agents=[
            {"agent_id": "other_agent", "reputation": 3.0, "resources": 80.0},
        ],
        pending_proposals=[],
        available_tasks=[
            {"task_id": "task_1", "prompt": "Complete this task", "reward": 5.0},
        ],
        active_tasks=[],
        can_post=True,
        can_interact=True,
        can_vote=True,
        can_claim_task=True,
        ecosystem_metrics={"toxicity": 0.1, "welfare": 50.0},
    )


@pytest.fixture
def mock_anthropic_response():
    """Mock Anthropic API response."""
    mock = MagicMock()
    mock.content = [
        MagicMock(
            text='{"action_type": "POST", "reasoning": "Test", "params": {"content": "Hello"}}'
        )
    ]
    mock.usage = MagicMock(input_tokens=100, output_tokens=50)
    return mock


# =============================================================================
# LLMConfig Tests
# =============================================================================


class TestLLMConfig:
    """Tests for LLMConfig."""

    def test_default_config(self):
        """Test default configuration values."""
        config = LLMConfig()
        assert config.provider == LLMProvider.ANTHROPIC
        assert config.model == "claude-sonnet-4-20250514"
        assert config.temperature == 0.7
        assert config.max_tokens == 512
        assert config.timeout == 30.0
        assert config.max_retries == 3

    def test_anthropic_config(self):
        """Test Anthropic provider configuration."""
        config = LLMConfig(
            provider=LLMProvider.ANTHROPIC,
            model="claude-3-haiku-20240307",
            api_key="test-key",
        )
        assert config.provider == LLMProvider.ANTHROPIC
        assert config.model == "claude-3-haiku-20240307"

    def test_openai_config(self):
        """Test OpenAI provider configuration."""
        config = LLMConfig(
            provider=LLMProvider.OPENAI,
            model="gpt-4o",
            api_key="test-key",
        )
        assert config.provider == LLMProvider.OPENAI
        assert config.model == "gpt-4o"

    def test_ollama_config(self):
        """Test Ollama provider configuration."""
        config = LLMConfig(
            provider=LLMProvider.OLLAMA,
            model="llama3",
        )
        assert config.provider == LLMProvider.OLLAMA
        assert config.base_url == "http://localhost:11434"

    def test_invalid_temperature(self):
        """Test temperature validation."""
        with pytest.raises(ValueError, match="temperature must be in"):
            LLMConfig(temperature=1.5)

        with pytest.raises(ValueError, match="temperature must be in"):
            LLMConfig(temperature=-0.1)

    def test_invalid_max_tokens(self):
        """Test max_tokens validation."""
        with pytest.raises(ValueError, match="max_tokens must be positive"):
            LLMConfig(max_tokens=0)

    def test_invalid_timeout(self):
        """Test timeout validation."""
        with pytest.raises(ValueError, match="timeout must be positive"):
            LLMConfig(timeout=-1)

    def test_persona_types(self):
        """Test all persona types are valid."""
        for persona in PersonaType:
            config = LLMConfig(persona=persona)
            assert config.persona == persona


class TestLLMUsageStats:
    """Tests for LLMUsageStats."""

    def test_initial_stats(self):
        """Test initial usage stats are zero."""
        stats = LLMUsageStats()
        assert stats.total_requests == 0
        assert stats.total_input_tokens == 0
        assert stats.total_output_tokens == 0
        assert stats.estimated_cost_usd == 0.0

    def test_record_usage(self):
        """Test recording usage updates stats."""
        stats = LLMUsageStats()
        stats.record_usage(
            model="claude-sonnet-4-20250514",
            input_tokens=1000,
            output_tokens=500,
        )
        assert stats.total_requests == 1
        assert stats.total_input_tokens == 1000
        assert stats.total_output_tokens == 500
        assert stats.estimated_cost_usd > 0

    def test_record_multiple_usage(self):
        """Test recording multiple API calls."""
        stats = LLMUsageStats()
        stats.record_usage("claude-sonnet-4-20250514", 100, 50)
        stats.record_usage("claude-sonnet-4-20250514", 200, 100)
        assert stats.total_requests == 2
        assert stats.total_input_tokens == 300
        assert stats.total_output_tokens == 150

    def test_record_failure(self):
        """Test recording failed requests."""
        stats = LLMUsageStats()
        stats.record_usage(
            model="claude-sonnet-4-20250514",
            input_tokens=0,
            output_tokens=0,
            retries=3,
            failed=True,
        )
        assert stats.total_failures == 1
        assert stats.total_retries == 3

    def test_to_dict(self):
        """Test conversion to dictionary."""
        stats = LLMUsageStats()
        stats.record_usage("gpt-4o", 1000, 500)
        d = stats.to_dict()
        assert "total_requests" in d
        assert "estimated_cost_usd" in d
        assert d["total_requests"] == 1


# =============================================================================
# LLM Prompts Tests
# =============================================================================


class TestLLMPrompts:
    """Tests for prompt templates and formatting."""

    def test_all_personas_have_prompts(self):
        """Test all persona types have defined prompts."""
        for persona in PersonaType:
            assert persona in PERSONA_PROMPTS
            assert len(PERSONA_PROMPTS[persona]) > 100

    def test_format_observation(self, basic_observation):
        """Test observation formatting."""
        formatted = format_observation(basic_observation)
        assert "test_agent" in formatted
        assert "Epoch: 1" in formatted
        assert "Reputation: 5.00" in formatted
        assert "Visible Posts" in formatted
        assert "Available Tasks" in formatted

    def test_format_observation_empty(self):
        """Test formatting with minimal observation."""
        obs = Observation(
            agent_state=AgentState(agent_id="empty"),
            current_epoch=0,
            current_step=0,
            visible_posts=[],
            visible_agents=[],
            pending_proposals=[],
            available_tasks=[],
            active_tasks=[],
            can_post=False,
            can_interact=False,
            can_vote=False,
            can_claim_task=False,
        )
        formatted = format_observation(obs)
        assert "empty" in formatted
        assert "Can post: False" in formatted

    def test_build_action_prompt(self, basic_observation):
        """Test building action prompt."""
        system, user = build_action_prompt(
            persona=PersonaType.HONEST,
            observation=basic_observation,
        )
        assert "cooperative agent" in system.lower()
        assert "action_type" in system
        assert "test_agent" in user
        assert "Your Turn" in user

    def test_build_action_prompt_with_custom_system(self, basic_observation):
        """Test building action prompt with custom system prompt."""
        custom = "You are a custom agent."
        system, user = build_action_prompt(
            persona=PersonaType.HONEST,
            observation=basic_observation,
            custom_system_prompt=custom,
        )
        assert system.startswith(custom)

    def test_build_accept_prompt(self, basic_observation):
        """Test building accept/reject prompt."""
        proposal = {
            "proposal_id": "prop_1",
            "initiator_id": "other_agent",
            "interaction_type": "collaboration",
            "content": "Let's work together",
        }
        system, user = build_accept_prompt(
            persona=PersonaType.STRATEGIC,
            proposal=proposal,
            observation=basic_observation,
        )
        assert "accept" in system.lower()
        assert "prop_1" in user
        assert "other_agent" in user


# =============================================================================
# LLMAgent Tests
# =============================================================================


class TestLLMAgent:
    """Tests for LLMAgent."""

    def test_agent_creation(self, basic_llm_config):
        """Test creating an LLM agent."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(
            agent_id="llm_1",
            llm_config=basic_llm_config,
        )
        assert agent.agent_id == "llm_1"
        assert agent.llm_config == basic_llm_config

    def test_agent_repr(self, basic_llm_config):
        """Test agent string representation."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)
        repr_str = repr(agent)
        assert "llm_1" in repr_str
        assert "anthropic" in repr_str

    def test_parse_action_response_json(self, basic_llm_config):
        """Test parsing JSON action response."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        response = '{"action_type": "NOOP", "reasoning": "Nothing to do"}'
        parsed = agent._parse_action_response(response)
        assert parsed["action_type"] == "NOOP"
        assert parsed["reasoning"] == "Nothing to do"

    def test_parse_action_response_markdown(self, basic_llm_config):
        """Test parsing JSON in markdown code block."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        response = """Here's my action:
```json
{"action_type": "POST", "params": {"content": "Hello"}}
```
"""
        parsed = agent._parse_action_response(response)
        assert parsed["action_type"] == "POST"
        assert parsed["params"]["content"] == "Hello"

    def test_parse_action_response_invalid(self, basic_llm_config):
        """Test parsing invalid response raises error."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        with pytest.raises(ValueError, match="No JSON found"):
            agent._parse_action_response("This has no JSON")

    def test_action_dict_to_action_noop(self, basic_llm_config):
        """Test converting NOOP action dict."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        action_dict = {"action_type": "NOOP"}
        action = agent._action_dict_to_action(action_dict)
        assert action.action_type == ActionType.NOOP

    def test_action_dict_to_action_post(self, basic_llm_config):
        """Test converting POST action dict."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        action_dict = {
            "action_type": "POST",
            "params": {"content": "Test post"},
        }
        action = agent._action_dict_to_action(action_dict)
        assert action.action_type == ActionType.POST
        assert action.content == "Test post"

    def test_action_dict_to_action_vote(self, basic_llm_config):
        """Test converting VOTE action dict."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        action_dict = {
            "action_type": "VOTE",
            "params": {"post_id": "post_1", "direction": -1},
        }
        action = agent._action_dict_to_action(action_dict)
        assert action.action_type == ActionType.VOTE
        assert action.target_id == "post_1"
        assert action.vote_direction == -1

    def test_action_dict_to_action_unknown(self, basic_llm_config):
        """Test unknown action type defaults to NOOP."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        action_dict = {"action_type": "UNKNOWN_ACTION"}
        action = agent._action_dict_to_action(action_dict)
        assert action.action_type == ActionType.NOOP

    @pytest.mark.asyncio
    async def test_act_async_with_mock(self, basic_llm_config, basic_observation):
        """Test async act method with mocked API."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        # Mock the async LLM call
        mock_response = '{"action_type": "POST", "reasoning": "Sharing info", "params": {"content": "Hello world"}}'
        agent._call_llm_async = AsyncMock(return_value=(mock_response, 100, 50))

        action = await agent.act_async(basic_observation)

        assert action.action_type == ActionType.POST
        assert action.content == "Hello world"
        agent._call_llm_async.assert_called_once()

    @pytest.mark.asyncio
    async def test_act_async_fallback_on_error(
        self, basic_llm_config, basic_observation
    ):
        """Test async act falls back to NOOP on error."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        # Mock API to raise error
        agent._call_llm_async = AsyncMock(side_effect=Exception("API Error"))

        action = await agent.act_async(basic_observation)

        assert action.action_type == ActionType.NOOP

    @pytest.mark.asyncio
    async def test_accept_interaction_async(self, basic_llm_config, basic_observation):
        """Test async accept_interaction method."""
        from swarm.agents.base import InteractionProposal
        from swarm.agents.llm_agent import LLMAgent
        from swarm.models.interaction import InteractionType

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        proposal = InteractionProposal(
            proposal_id="prop_1",
            initiator_id="other_agent",
            counterparty_id="llm_1",
            interaction_type=InteractionType.COLLABORATION,
            content="Let's collaborate",
        )

        # Mock acceptance
        mock_response = '{"accept": true, "reasoning": "Looks beneficial"}'
        agent._call_llm_async = AsyncMock(return_value=(mock_response, 100, 50))

        accept = await agent.accept_interaction_async(proposal, basic_observation)

        assert accept is True

    @pytest.mark.asyncio
    async def test_accept_interaction_async_reject(
        self, basic_llm_config, basic_observation
    ):
        """Test async accept_interaction rejection."""
        from swarm.agents.base import InteractionProposal
        from swarm.agents.llm_agent import LLMAgent
        from swarm.models.interaction import InteractionType

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        proposal = InteractionProposal(
            proposal_id="prop_1",
            initiator_id="other_agent",
            counterparty_id="llm_1",
            interaction_type=InteractionType.COLLABORATION,
            content="Suspicious proposal",
        )

        # Mock rejection
        mock_response = '{"accept": false, "reasoning": "Too risky"}'
        agent._call_llm_async = AsyncMock(return_value=(mock_response, 100, 50))

        accept = await agent.accept_interaction_async(proposal, basic_observation)

        assert accept is False

    def test_get_usage_stats(self, basic_llm_config):
        """Test getting usage statistics."""
        from swarm.agents.llm_agent import LLMAgent

        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)

        # Manually record some usage
        agent.usage_stats.record_usage("claude-sonnet-4-20250514", 500, 200)

        stats = agent.get_usage_stats()
        assert stats["total_requests"] == 1
        assert stats["total_input_tokens"] == 500


# =============================================================================
# New Provider Tests (Groq, Together, DeepSeek, Google)
# =============================================================================


class TestNewProviderEnums:
    """Tests for new LLM provider enum values."""

    @pytest.mark.parametrize(
        "provider,value",
        [
            (LLMProvider.GROQ, "groq"),
            (LLMProvider.TOGETHER, "together"),
            (LLMProvider.DEEPSEEK, "deepseek"),
            (LLMProvider.GOOGLE, "google"),
        ],
    )
    def test_provider_enum_values(self, provider, value):
        """Test new provider enum values are correct strings."""
        assert provider.value == value

    def test_provider_round_trip(self):
        """Test creating providers from string values."""
        for name in ("groq", "together", "deepseek", "google"):
            assert LLMProvider(name).value == name


class TestNewProviderBaseUrls:
    """Tests for default base_url assignment in LLMConfig.__post_init__."""

    def test_groq_default_base_url(self):
        config = LLMConfig(provider=LLMProvider.GROQ, api_key="test")
        assert config.base_url == "https://api.groq.com/openai/v1"

    def test_together_default_base_url(self):
        config = LLMConfig(provider=LLMProvider.TOGETHER, api_key="test")
        assert config.base_url == "https://api.together.xyz/v1"

    def test_deepseek_default_base_url(self):
        config = LLMConfig(provider=LLMProvider.DEEPSEEK, api_key="test")
        assert config.base_url == "https://api.deepseek.com/v1"

    def test_google_no_default_base_url(self):
        config = LLMConfig(provider=LLMProvider.GOOGLE, api_key="test")
        assert config.base_url is None

    def test_custom_base_url_not_overwritten(self):
        config = LLMConfig(
            provider=LLMProvider.GROQ,
            api_key="test",
            base_url="https://custom.example.com/v1",
        )
        assert config.base_url == "https://custom.example.com/v1"


class TestNewProviderApiKeys:
    """Tests for API key resolution from environment variables."""

    @pytest.mark.parametrize(
        "provider,env_var",
        [
            (LLMProvider.GROQ, "GROQ_API_KEY"),
            (LLMProvider.TOGETHER, "TOGETHER_API_KEY"),
            (LLMProvider.DEEPSEEK, "DEEPSEEK_API_KEY"),
            (LLMProvider.GOOGLE, "GOOGLE_API_KEY"),
        ],
    )
    def test_api_key_from_env(self, provider, env_var):
        """Test API key is read from the correct environment variable."""
        from swarm.agents.llm_agent import LLMAgent

        with patch.dict("os.environ", {env_var: "test-secret-key"}, clear=False):
            config = LLMConfig(provider=provider)
            agent = LLMAgent(agent_id="test", llm_config=config)
            assert agent._api_key == "test-secret-key"


class TestNewProviderDispatch:
    """Tests for _call_llm_async dispatch to correct methods."""

    @pytest.mark.asyncio
    async def test_groq_dispatches_to_openai_compatible(self):
        from swarm.agents.llm_agent import LLMAgent

        config = LLMConfig(provider=LLMProvider.GROQ, api_key="test")
        agent = LLMAgent(agent_id="test", llm_config=config)
        agent._call_openai_compatible_async = AsyncMock(
            return_value=("resp", 10, 5)
        )
        result = await agent._call_llm_async("sys", "usr")
        agent._call_openai_compatible_async.assert_called_once_with("sys", "usr")
        assert result == ("resp", 10, 5)

    @pytest.mark.asyncio
    async def test_together_dispatches_to_openai_compatible(self):
        from swarm.agents.llm_agent import LLMAgent

        config = LLMConfig(provider=LLMProvider.TOGETHER, api_key="test")
        agent = LLMAgent(agent_id="test", llm_config=config)
        agent._call_openai_compatible_async = AsyncMock(
            return_value=("resp", 10, 5)
        )
        result = await agent._call_llm_async("sys", "usr")
        agent._call_openai_compatible_async.assert_called_once_with("sys", "usr")
        assert result == ("resp", 10, 5)

    @pytest.mark.asyncio
    async def test_deepseek_dispatches_to_openai_compatible(self):
        from swarm.agents.llm_agent import LLMAgent

        config = LLMConfig(provider=LLMProvider.DEEPSEEK, api_key="test")
        agent = LLMAgent(agent_id="test", llm_config=config)
        agent._call_openai_compatible_async = AsyncMock(
            return_value=("resp", 10, 5)
        )
        result = await agent._call_llm_async("sys", "usr")
        agent._call_openai_compatible_async.assert_called_once_with("sys", "usr")
        assert result == ("resp", 10, 5)

    @pytest.mark.asyncio
    async def test_google_dispatches_to_google_async(self):
        from swarm.agents.llm_agent import LLMAgent

        config = LLMConfig(provider=LLMProvider.GOOGLE, api_key="test")
        agent = LLMAgent(agent_id="test", llm_config=config)
        agent._call_google_async = AsyncMock(return_value=("resp", 10, 5))
        result = await agent._call_llm_async("sys", "usr")
        agent._call_google_async.assert_called_once_with("sys", "usr")
        assert result == ("resp", 10, 5)


class TestNewProviderCostTracking:
    """Tests for cost tracking entries for new provider models."""

    @pytest.mark.parametrize(
        "model",
        [
            "llama-3.3-70b-versatile",
            "mixtral-8x7b-32768",
            "meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo",
            "deepseek-chat",
            "deepseek-reasoner",
            "gemini-2.0-flash",
            "gemini-1.5-pro",
        ],
    )
    def test_model_has_cost_entries(self, model):
        """Test that new models have cost entries in LLMUsageStats."""
        stats = LLMUsageStats()
        assert model in stats._COST_PER_1M_INPUT
        assert model in stats._COST_PER_1M_OUTPUT

    def test_cost_calculation_deepseek(self):
        """Test cost calculation for a DeepSeek model."""
        stats = LLMUsageStats()
        stats.record_usage("deepseek-chat", input_tokens=1_000_000, output_tokens=1_000_000)
        # $0.14 input + $0.28 output = $0.42
        assert abs(stats.estimated_cost_usd - 0.42) < 0.01

    def test_cost_calculation_gemini(self):
        """Test cost calculation for a Gemini model."""
        stats = LLMUsageStats()
        stats.record_usage("gemini-2.0-flash", input_tokens=1_000_000, output_tokens=1_000_000)
        # $0.10 input + $0.40 output = $0.50
        assert abs(stats.estimated_cost_usd - 0.50) < 0.01


# =============================================================================
# Scenario Loader Integration Tests
# =============================================================================


class TestScenarioLoaderLLM:
    """Tests for scenario loader LLM agent support."""

    def test_parse_llm_config(self):
        """Test parsing LLM config from YAML-like dict."""
        from swarm.scenarios.loader import parse_llm_config

        data = {
            "provider": "anthropic",
            "model": "claude-3-haiku-20240307",
            "persona": "strategic",
            "temperature": 0.5,
        }

        config = parse_llm_config(data)

        assert config.provider == LLMProvider.ANTHROPIC
        assert config.model == "claude-3-haiku-20240307"
        assert config.persona == PersonaType.STRATEGIC
        assert config.temperature == 0.5

    def test_parse_llm_config_defaults(self):
        """Test parsing LLM config with defaults."""
        from swarm.scenarios.loader import parse_llm_config

        config = parse_llm_config({})

        assert config.provider == LLMProvider.ANTHROPIC
        assert config.persona == PersonaType.OPEN

    def test_parse_llm_config_invalid_provider(self):
        """Test parsing with invalid provider raises error."""
        from swarm.scenarios.loader import parse_llm_config

        with pytest.raises(ValueError, match="Unknown LLM provider"):
            parse_llm_config({"provider": "invalid_provider"})

    def test_parse_llm_config_invalid_persona(self):
        """Test parsing with invalid persona raises error."""
        from swarm.scenarios.loader import parse_llm_config

        with pytest.raises(ValueError, match="Unknown persona type"):
            parse_llm_config({"persona": "invalid_persona"})

    def test_create_llm_agents(self):
        """Test creating LLM agents from specs."""
        from swarm.agents.llm_agent import LLMAgent
        from swarm.scenarios.loader import create_agents

        specs = [
            {
                "type": "llm",
                "count": 2,
                "llm": {
                    "provider": "anthropic",
                    "model": "claude-sonnet-4-20250514",
                    "persona": "honest",
                },
            },
        ]

        agents = create_agents(specs)

        assert len(agents) == 2
        assert all(isinstance(a, LLMAgent) for a in agents)
        assert agents[0].agent_id == "llm_1"
        assert agents[1].agent_id == "llm_2"

    def test_create_mixed_agents(self):
        """Test creating mixed scripted and LLM agents."""
        from swarm.agents.honest import HonestAgent
        from swarm.agents.llm_agent import LLMAgent
        from swarm.scenarios.loader import create_agents

        specs = [
            {"type": "honest", "count": 2},
            {
                "type": "llm",
                "count": 1,
                "llm": {"provider": "openai", "model": "gpt-4o"},
            },
            {"type": "adversarial", "count": 1},
        ]

        agents = create_agents(specs)

        assert len(agents) == 4
        assert isinstance(agents[0], HonestAgent)
        assert isinstance(agents[1], HonestAgent)
        assert isinstance(agents[2], LLMAgent)
        assert agents[2].llm_config.provider == LLMProvider.OPENAI


# =============================================================================
# Async Orchestrator Tests
# =============================================================================


class TestAsyncOrchestrator:
    """Tests for async orchestrator functionality."""

    @pytest.mark.asyncio
    async def test_run_async_with_scripted_agents(self):
        """Test async run with only scripted agents."""
        from swarm.agents.honest import HonestAgent
        from swarm.core.orchestrator import Orchestrator, OrchestratorConfig

        config = OrchestratorConfig(n_epochs=2, steps_per_epoch=3)
        orchestrator = Orchestrator(config)

        # Add scripted agents
        for i in range(3):
            orchestrator.register_agent(HonestAgent(f"honest_{i}"))

        metrics = await orchestrator.run_async()

        assert len(metrics) == 2
        assert all(m.epoch >= 0 for m in metrics)

    @pytest.mark.asyncio
    async def test_run_async_with_llm_agents(self, basic_llm_config):
        """Test async run with LLM agents (mocked)."""
        from swarm.agents.llm_agent import LLMAgent
        from swarm.core.orchestrator import Orchestrator, OrchestratorConfig

        config = OrchestratorConfig(n_epochs=1, steps_per_epoch=2)
        orchestrator = Orchestrator(config)

        # Create LLM agent with mocked API
        agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)
        mock_response = '{"action_type": "NOOP", "reasoning": "Waiting"}'
        agent._call_llm_async = AsyncMock(return_value=(mock_response, 50, 20))

        orchestrator.register_agent(agent)

        metrics = await orchestrator.run_async()

        assert len(metrics) == 1

    def test_is_llm_agent(self, basic_llm_config):
        """Test LLM agent detection."""
        from swarm.agents.honest import HonestAgent
        from swarm.agents.llm_agent import LLMAgent
        from swarm.core.orchestrator import Orchestrator, OrchestratorConfig

        config = OrchestratorConfig()
        orchestrator = Orchestrator(config)

        llm_agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)
        scripted_agent = HonestAgent(agent_id="honest_1")

        assert orchestrator._is_llm_agent(llm_agent) is True
        assert orchestrator._is_llm_agent(scripted_agent) is False

    def test_get_llm_usage_stats(self, basic_llm_config):
        """Test getting LLM usage stats from orchestrator."""
        from swarm.agents.honest import HonestAgent
        from swarm.agents.llm_agent import LLMAgent
        from swarm.core.orchestrator import Orchestrator, OrchestratorConfig

        config = OrchestratorConfig()
        orchestrator = Orchestrator(config)

        llm_agent = LLMAgent(agent_id="llm_1", llm_config=basic_llm_config)
        llm_agent.usage_stats.record_usage("claude-sonnet-4-20250514", 100, 50)

        scripted_agent = HonestAgent(agent_id="honest_1")

        orchestrator.register_agent(llm_agent)
        orchestrator.register_agent(scripted_agent)

        stats = orchestrator.get_llm_usage_stats()

        assert "llm_1" in stats
        assert "honest_1" not in stats
        assert stats["llm_1"]["total_requests"] == 1
