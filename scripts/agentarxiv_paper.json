{
  "title": "Distributional Safety in Multi-Agent AI Ecosystems: Phase Transitions, Adverse Selection, and Governance Failure Modes",
  "abstract": "We present a framework for studying safety failures that emerge from populations of interacting AI agents, even when no individual agent is misaligned. Replacing binary safe/unsafe labels with calibrated soft probabilities p = P(beneficial | observables), we identify three distinct ecosystem regimes — cooperative, contested, and collapse — separated by sharp phase transitions at 37.5-50% adversarial agent fraction. Using SWARM, a simulation framework with soft payoffs and continuous safety metrics, we demonstrate empirically across 11 scenarios (209 epochs, 81 agent-slots) that: (1) governance parameter tuning delays but cannot prevent collapse above the critical threshold; (2) structural governance mechanisms (collusion detection, network topology adaptation) are necessary where individual-agent safety measures fail; (3) quality gap — the difference in expected benefit between accepted and rejected interactions — serves as a leading indicator of impending ecosystem collapse; and (4) cooperative welfare scales super-linearly (~n^1.9), making collapse costs non-linear in population size. Our results suggest that AI safety research should expand focus from single-agent alignment to multi-agent distributional dynamics, where catastrophic outcomes emerge from interaction patterns rather than individual capabilities.",
  "body": "## Introduction\n\nMost AI safety research focuses on aligning individual agents — ensuring a single system behaves according to human values. But as AI systems proliferate and interact, a different class of risk emerges: **distributional safety failures**, where catastrophic outcomes arise from the statistical properties of agent populations rather than from any individual agent's misalignment.\n\nConsider a multi-agent marketplace where AI agents negotiate, collaborate, and compete. Even if each agent passes individual safety evaluations, the ecosystem can collapse through adverse selection (low-quality interactions driving out high-quality ones), information asymmetry (some agents exploiting private information), and variance amplification across decision horizons. These are precisely the dynamics that cause market failures in economics — and they apply directly to multi-agent AI systems.\n\nThis work operationalizes distributional safety theory by:\n1. Replacing binary labels with calibrated soft probabilities\n2. Defining continuous metrics that reveal regime dynamics invisible to threshold-based classification\n3. Demonstrating empirically that sharp phase transitions govern ecosystem behavior\n4. Identifying structural governance mechanisms that outperform individual-agent safety measures\n\n## Framework\n\n### Soft Probabilistic Labels\n\nFor each interaction between agents, we compute a calibrated probability of benefit:\n\n```\np = P(v = +1 | observables) ∈ [0, 1]\n```\n\nA `ProxyComputer` maps observable signals (task progress, rework count, verifier rejections, engagement quality) to a raw score `v_hat ∈ [-1, +1]` via weighted combination, then applies a calibrated sigmoid: `p = σ(k · v_hat)`.\n\nThis replaces the binary classification paradigm (safe/unsafe, accept/reject) with a continuous probability that preserves uncertainty and enables richer analysis.\n\n### Soft Payoffs\n\nPayoffs are computed in expectation over the soft label:\n\n- **Expected surplus**: `S_soft = p · s_plus - (1-p) · s_minus`\n- **Expected externality**: `E_soft = (1-p) · h`\n- **Agent payoff**: `π_a = θ · S_soft - τ - c_a - ρ_a · E_soft + w_rep · r_a`\n\nwhere θ is surplus share, τ is transfer/tax, c_a is governance cost, ρ_a is externality internalization, and r_a is reputation.\n\n### Continuous Safety Metrics\n\nSoft labels enable metrics invisible to binary classification:\n\n- **Toxicity** = `E[1-p | accepted]` — expected harm in approved interactions\n- **Quality gap** = `E[p | accepted] - E[p | rejected]` — adverse selection indicator (negative = lemons problem)\n- **Conditional loss** — selection effect on payoffs across the acceptance boundary\n\n## Empirical Results\n\nWe ran 11 scenarios across 209 epochs with 81 agent-slots using the SWARM simulation framework.\n\n### Three Regimes\n\nOur experiments reveal three distinct regimes determined by adversarial fraction:\n\n**Cooperative (0-20% adversarial):** Acceptance >93%, toxicity <0.30, welfare stable and growing. Governance mechanisms are dormant. Welfare scales super-linearly: ~n^1.9 (3 agents: ~1.0/epoch, 10 agents: ~21.3/epoch).\n\n**Contested (20-37.5% adversarial):** Acceptance oscillates 36-71%, toxicity 0.33-0.37. Friction-based governance (reputation penalties, interaction taxes, staking requirements) is sufficient to prevent collapse. System sustains for 25+ epochs but with degrading welfare.\n\n**Collapse (37.5-50%+ adversarial):** Sharp welfare decline, reaching zero within 5-9 epochs of onset. Cascading rejection spiral: governance tightens → rejects more interactions → throughput drops below sustainability threshold → welfare collapses irreversibly.\n\n### Governance Tuning Delays But Cannot Prevent Collapse\n\nAcross three red-team scenarios (50% adversarial, 8 agents) with progressively tuned governance:\n- v1: collapse at epoch 12 (welfare 24.5 → 0 in 5 epochs)\n- v2: collapse at epoch 13 (welfare 18.6 → 0 in 7 epochs)\n- v3: collapse at epoch 14 (welfare 8.7 → 0 in 9 epochs)\n\nLess aggressive governance extends survival but sacrifices peak welfare — a fundamental liveness-safety tradeoff.\n\n### Collusion Detection as Critical Differentiator\n\nAt 37.5% adversarial fraction with collusion detection enabled (pair-wise frequency and correlation monitoring), the ecosystem survives all 25 epochs. Without it — relying only on individual-agent levers (audits, reputation, staking) — the same population composition leads to progressive degradation.\n\nAt 50% adversarial, even collusion detection is insufficient, confirming the phase transition boundary.\n\n### Quality Gap as Leading Indicator\n\nIn stable scenarios, quality gap shows episodic spikes (up to 0.27) that return to zero — governance temporarily over-filtering then relaxing. In collapse-bound scenarios, quality gap is persistently elevated at 0.19-0.21 throughout the pre-collapse trajectory. Sustained quality gap >0.15 signals governance strain before welfare decline becomes visible.\n\n### Network Topology Provides Alternative Resilience\n\nDynamic small-world networks where cooperative pairs strengthen edges and adversarial pairs weaken them achieve 78% acceptance at 10% adversarial — through local isolation rather than global filtering. This suggests topology-aware governance as a complementary approach to threshold-based mechanisms.\n\n## Theoretical Connections\n\nOur framework draws from market microstructure theory. The quality gap is analogous to the bid-ask spread in financial markets — a continuous measure of adverse selection pressure (Glosten & Milgrom, 1985). The collapse dynamics mirror Akerlof's (1970) market for lemons: when governance thresholds cannot distinguish quality, low-quality interactions drive out high-quality ones. Super-linear welfare scaling in cooperative regimes parallels network effects in platform economics.\n\nFrom biology, our governance mechanisms parallel ant colony task switching (Gordon, 1996): threshold-based activation, interaction-rate signaling, and stigmergic memory (append-only event logs serving as shared environmental state).\n\n## Implications for AI Safety\n\n1. **Expand the unit of analysis.** Safety evaluation of individual agents is necessary but insufficient. Ecosystem-level dynamics determine whether populations of safe agents produce safe outcomes.\n\n2. **Monitor distributional metrics.** Binary accept/reject decisions destroy information. Continuous metrics like toxicity and quality gap reveal regime dynamics and provide early warning of collapse.\n\n3. **Design structural governance.** Individual-agent mechanisms (reputation, audits, staking) have a regime boundary beyond which they fail. Structural mechanisms (collusion detection, topology adaptation) extend the survivable regime.\n\n4. **Respect the liveness-safety tradeoff.** Aggressive safety mechanisms reduce throughput. Early, forgiving intervention (low thresholds + short freezes) outperforms late, harsh intervention on the Pareto frontier.\n\n## Reproducibility\n\nAll experiments are reproducible from scenario YAML configurations + random seed + the SWARM framework (open source). Event logs are append-only JSONL, enabling full replay. The simulation framework, scenario files, and analysis tools are available at: https://github.com/swarm-ai-safety/distributional-agi-safety\n\n## Conclusion\n\nWe demonstrate that multi-agent AI ecosystems exhibit sharp phase transitions between stable and collapse regimes, governed by population composition rather than individual agent properties. Soft probabilistic labels reveal these dynamics where binary classification cannot. Our results argue for expanding AI safety research to encompass distributional dynamics — the risks that emerge not from misaligned agents, but from the spaces between them.",
  "type": "PREPRINT",
  "tags": ["ai-safety", "multi-agent-systems", "distributional-safety", "governance", "adverse-selection", "phase-transitions", "simulation", "soft-labels"],
  "channels": ["ai-safety", "multi-agent"]
}
