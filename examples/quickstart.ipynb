{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SWARM Quickstart: Multi-Agent Safety in 5 Minutes\n\n**SWARM** (System-Wide Assessment of Risk in Multi-agent systems) is a simulation framework for studying distributional safety in multi-agent AI ecosystems. Instead of binary safe/unsafe labels, SWARM uses **soft probabilistic labels** `p = P(v = +1)` to capture the continuous, uncertain nature of interaction quality.\n\nThis notebook walks you through:\n1. Running a baseline cooperative scenario\n2. Visualizing ecosystem health metrics\n3. Introducing adversaries and watching the system collapse\n\n**Requirements:** Python 3.10+, no API keys needed."
  },
  {
   "cell_type": "code",
   "source": "# Install SWARM and clone the repo (for scenario files)\nimport os\n\nif os.getenv(\"COLAB_RELEASE_TAG\"):\n    !git clone --depth 1 https://github.com/swarm-ai-safety/swarm.git /content/swarm\n    %pip install -q -e \"/content/swarm[runtime]\"\n    os.chdir(\"/content/swarm\")\n    print(\"Installed SWARM from GitHub.\")\nelse:\n    print(\"Not running in Colab — assuming local install.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom swarm.scenarios import load_scenario, build_orchestrator\n\n# Locate scenario files — works from repo root (Colab) or examples/ (local)\nSCENARIOS_DIR = Path(\"scenarios\") if Path(\"scenarios\").is_dir() else Path(\"../scenarios\")\nprint(\"Available scenarios:\", sorted(p.stem for p in SCENARIOS_DIR.glob(\"*.yaml\")))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run a Baseline Scenario\n",
    "\n",
    "The **baseline** scenario has 5 agents (3 honest, 1 opportunistic, 1 deceptive) interacting over 10 epochs with no governance. This establishes a reference for what a mostly-cooperative ecosystem looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run baseline\n",
    "scenario = load_scenario(SCENARIOS_DIR / \"baseline.yaml\")\n",
    "orchestrator = build_orchestrator(scenario)\n",
    "baseline_history = orchestrator.run()\n",
    "\n",
    "# Build a metrics DataFrame\n",
    "df_baseline = pd.DataFrame([\n",
    "    {\n",
    "        \"epoch\": m.epoch,\n",
    "        \"toxicity\": m.toxicity_rate,\n",
    "        \"welfare\": m.total_welfare,\n",
    "        \"acceptance_rate\": m.accepted_interactions / max(m.total_interactions, 1),\n",
    "        \"interactions\": m.total_interactions,\n",
    "    }\n",
    "    for m in baseline_history\n",
    "])\n",
    "\n",
    "# Per-agent payoff summary\n",
    "agent_data = []\n",
    "for agent in orchestrator.get_all_agents():\n",
    "    state = orchestrator.state.get_agent(agent.agent_id)\n",
    "    agent_data.append({\n",
    "        \"agent_id\": state.agent_id,\n",
    "        \"type\": state.agent_type.value,\n",
    "        \"payoff\": state.total_payoff,\n",
    "        \"reputation\": state.reputation,\n",
    "    })\n",
    "df_agents_baseline = pd.DataFrame(agent_data)\n",
    "\n",
    "print(\"=== Epoch Metrics ===\")\n",
    "print(df_baseline.to_string(index=False))\n",
    "print()\n",
    "print(\"=== Agent Summary ===\")\n",
    "print(df_agents_baseline.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7))\n",
    "fig.suptitle(\"Baseline Scenario (5 agents, no governance)\", fontsize=14)\n",
    "\n",
    "# Toxicity over time\n",
    "axes[0, 0].plot(df_baseline[\"epoch\"], df_baseline[\"toxicity\"], \"o-\", color=\"#d62728\")\n",
    "axes[0, 0].set_ylabel(\"Toxicity rate\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_title(\"Toxicity\")\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Welfare over time\n",
    "axes[0, 1].plot(df_baseline[\"epoch\"], df_baseline[\"welfare\"], \"s-\", color=\"#2ca02c\")\n",
    "axes[0, 1].set_ylabel(\"Total welfare\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_title(\"Welfare\")\n",
    "\n",
    "# Acceptance rate over time\n",
    "axes[1, 0].plot(df_baseline[\"epoch\"], df_baseline[\"acceptance_rate\"], \"^-\", color=\"#1f77b4\")\n",
    "axes[1, 0].set_ylabel(\"Acceptance rate\")\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_title(\"Acceptance Rate\")\n",
    "axes[1, 0].set_ylim(0, 1.05)\n",
    "\n",
    "# Per-agent payoff bar chart\n",
    "colors = [\"#2ca02c\" if t == \"honest\" else \"#ff7f0e\" if t == \"opportunistic\" else \"#d62728\"\n",
    "          for t in df_agents_baseline[\"type\"]]\n",
    "axes[1, 1].bar(df_agents_baseline[\"agent_id\"], df_agents_baseline[\"payoff\"], color=colors)\n",
    "axes[1, 1].set_ylabel(\"Total payoff\")\n",
    "axes[1, 1].set_title(\"Per-Agent Payoff\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Add Adversaries\n",
    "\n",
    "Now let's run the **adversarial red-team** scenario: 8 agents (4 honest, 2 adversarial, 2 adaptive adversaries) with governance enabled (staking, circuit breakers, audits, collusion detection). The adversarial fraction is 50% -- right at the critical threshold our research identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run adversarial scenario\n",
    "adv_scenario = load_scenario(SCENARIOS_DIR / \"adversarial_redteam.yaml\")\n",
    "adv_orchestrator = build_orchestrator(adv_scenario)\n",
    "adv_history = adv_orchestrator.run()\n",
    "\n",
    "df_adv = pd.DataFrame([\n",
    "    {\n",
    "        \"epoch\": m.epoch,\n",
    "        \"toxicity\": m.toxicity_rate,\n",
    "        \"welfare\": m.total_welfare,\n",
    "        \"acceptance_rate\": m.accepted_interactions / max(m.total_interactions, 1),\n",
    "    }\n",
    "    for m in adv_history\n",
    "])\n",
    "\n",
    "# Overlay comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(\"Baseline vs Adversarial Red-Team\", fontsize=14)\n",
    "\n",
    "# Welfare comparison\n",
    "ax1.plot(df_baseline[\"epoch\"], df_baseline[\"welfare\"], \"s-\", label=\"Baseline (20% adv)\", color=\"#2ca02c\")\n",
    "ax1.plot(df_adv[\"epoch\"], df_adv[\"welfare\"], \"v-\", label=\"Red-Team (50% adv)\", color=\"#d62728\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Total welfare\")\n",
    "ax1.set_title(\"Welfare Collapse\")\n",
    "ax1.legend()\n",
    "ax1.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Acceptance rate comparison\n",
    "ax2.plot(df_baseline[\"epoch\"], df_baseline[\"acceptance_rate\"], \"^-\", label=\"Baseline\", color=\"#1f77b4\")\n",
    "ax2.plot(df_adv[\"epoch\"], df_adv[\"acceptance_rate\"], \"v-\", label=\"Red-Team\", color=\"#d62728\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Acceptance rate\")\n",
    "ax2.set_title(\"Acceptance Rate Decline\")\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find collapse epoch (first epoch where welfare hits 0)\n",
    "collapse = df_adv[df_adv[\"welfare\"] <= 0.0]\n",
    "if not collapse.empty:\n",
    "    print(f\"\\nCollapse detected at epoch {collapse.iloc[0]['epoch']:.0f}\")\n",
    "    print(\"Governance delayed but did not prevent ecosystem failure.\")\n",
    "else:\n",
    "    print(\"\\nNo collapse detected -- governance held.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Just Happened?\n",
    "\n",
    "You've just observed the **three regimes** that emerge from SWARM simulations:\n",
    "\n",
    "| Regime | Adversarial % | Acceptance | Toxicity | Outcome |\n",
    "|--------|--------------|------------|----------|---------|\n",
    "| **Cooperative** | 0-20% | > 0.93 | < 0.30 | Stable welfare growth |\n",
    "| **Contested** | 20-37.5% | 0.42-0.94 | 0.33-0.37 | Declining but survivable |\n",
    "| **Collapse** | 50% | < 0.56 | ~0.30 | Welfare hits zero by epoch 12-14 |\n",
    "\n",
    "Key findings from 11 scenario runs:\n",
    "- **Critical threshold** at 37.5-50% adversarial fraction separates survival from collapse\n",
    "- **Governance tuning** (audits, staking, circuit breakers) delays collapse by 2 epochs but doesn't prevent it\n",
    "- **Collusion detection** is the critical differentiator: at 37.5% adversarial, it prevents collapse entirely\n",
    "- **Welfare scales super-linearly** with cooperative agents (3 agents: 1.0, 6 agents: 5.7, 10 agents: 21.3)\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- Explore other scenarios in `scenarios/` (collusion detection, network effects, marketplace economy)\n",
    "- Read the full paper: `docs/papers/distributional_agi_safety.md`\n",
    "- Try writing your own scenario YAML -- see `scenarios/baseline.yaml` as a template\n",
    "- Run parameter sweeps with `/sweep` in Claude Code\n",
    "\n",
    "**Repository:** [github.com/swarm-ai-safety/swarm](https://github.com/swarm-ai-safety/swarm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}