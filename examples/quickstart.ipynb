{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/swarm-ai-safety/swarm/blob/main/examples/quickstart.ipynb)\n\n# SWARM Quickstart: Multi-Agent Safety in 5 Minutes\n\n**SWARM** (System-Wide Assessment of Risk in Multi-agent systems) is a simulation framework for studying distributional safety in multi-agent AI ecosystems. Instead of binary safe/unsafe labels, SWARM uses **soft probabilistic labels** `p = P(v = +1)` to capture the continuous, uncertain nature of interaction quality.\n\n**No API keys required. Runs entirely locally (or in Colab).**\n\n## What you'll learn\n\n| Step | What happens | Key concept |\n|------|-------------|-------------|\n| 1 | Run a **baseline** cooperative scenario | Soft metrics: toxicity, welfare, acceptance |\n| 2 | Visualize ecosystem health | Per-agent payoffs and time-series diagnostics |\n| 3 | Inject **adversaries** and watch collapse | Phase transition at ~50% adversarial fraction |\n\n## Prerequisites\n\n- **Python 3.10+** (Colab satisfies this automatically)\n- Basic familiarity with multi-agent systems or AI safety concepts is helpful but not required\n\n## Key terms\n\n- **p**: Probability an interaction is beneficial, always in [0, 1]\n- **Toxicity rate**: Expected harm among accepted interactions — `E[1-p | accepted]`\n- **Quality gap**: Whether the system preferentially accepts low-quality interactions (adverse selection)\n- **Welfare**: System-wide surplus minus costs"
  },
  {
   "cell_type": "code",
   "source": "# --- Setup ---\n# This cell handles installation automatically.\n# In Colab: clones the repo and installs SWARM.\n# Locally: assumes you've already run `pip install -e \".[runtime]\"`.\nimport os\n\nif os.getenv(\"COLAB_RELEASE_TAG\"):\n    !git clone --depth 1 https://github.com/swarm-ai-safety/swarm.git /content/swarm\n    %pip install -q -e \"/content/swarm[runtime]\"\n    os.chdir(\"/content/swarm\")\n    print(\"Installed SWARM from GitHub. Ready to go!\")\nelse:\n    print(\"Local environment detected — using existing install.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nfrom swarm.scenarios import load_scenario, build_orchestrator\n\n# Locate scenario files — works from repo root (Colab) or examples/ (local)\nSCENARIOS_DIR = Path(\"scenarios\") if Path(\"scenarios\").is_dir() else Path(\"../scenarios\")\nprint(\"Available scenarios:\", sorted(p.stem for p in SCENARIOS_DIR.glob(\"*.yaml\")))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Run a Baseline Scenario\n\nThe **baseline** scenario has 5 agents (3 honest, 1 opportunistic, 1 deceptive) interacting over 10 epochs with no governance. This establishes a reference for what a mostly-cooperative ecosystem looks like.\n\n**What to look for:** Toxicity should stay low (~0.2-0.3) and welfare should grow steadily. The deceptive agent may earn slightly more by exploiting trust — that's expected and is exactly the kind of emergent dynamic SWARM is designed to surface."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run baseline\n",
    "scenario = load_scenario(SCENARIOS_DIR / \"baseline.yaml\")\n",
    "orchestrator = build_orchestrator(scenario)\n",
    "baseline_history = orchestrator.run()\n",
    "\n",
    "# Build a metrics DataFrame\n",
    "df_baseline = pd.DataFrame([\n",
    "    {\n",
    "        \"epoch\": m.epoch,\n",
    "        \"toxicity\": m.toxicity_rate,\n",
    "        \"welfare\": m.total_welfare,\n",
    "        \"acceptance_rate\": m.accepted_interactions / max(m.total_interactions, 1),\n",
    "        \"interactions\": m.total_interactions,\n",
    "    }\n",
    "    for m in baseline_history\n",
    "])\n",
    "\n",
    "# Per-agent payoff summary\n",
    "agent_data = []\n",
    "for agent in orchestrator.get_all_agents():\n",
    "    state = orchestrator.state.get_agent(agent.agent_id)\n",
    "    agent_data.append({\n",
    "        \"agent_id\": state.agent_id,\n",
    "        \"type\": state.agent_type.value,\n",
    "        \"payoff\": state.total_payoff,\n",
    "        \"reputation\": state.reputation,\n",
    "    })\n",
    "df_agents_baseline = pd.DataFrame(agent_data)\n",
    "\n",
    "print(\"=== Epoch Metrics ===\")\n",
    "print(df_baseline.to_string(index=False))\n",
    "print()\n",
    "print(\"=== Agent Summary ===\")\n",
    "print(df_agents_baseline.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(10, 7))\n",
    "fig.suptitle(\"Baseline Scenario (5 agents, no governance)\", fontsize=14)\n",
    "\n",
    "# Toxicity over time\n",
    "axes[0, 0].plot(df_baseline[\"epoch\"], df_baseline[\"toxicity\"], \"o-\", color=\"#d62728\")\n",
    "axes[0, 0].set_ylabel(\"Toxicity rate\")\n",
    "axes[0, 0].set_xlabel(\"Epoch\")\n",
    "axes[0, 0].set_title(\"Toxicity\")\n",
    "axes[0, 0].set_ylim(0, 1)\n",
    "\n",
    "# Welfare over time\n",
    "axes[0, 1].plot(df_baseline[\"epoch\"], df_baseline[\"welfare\"], \"s-\", color=\"#2ca02c\")\n",
    "axes[0, 1].set_ylabel(\"Total welfare\")\n",
    "axes[0, 1].set_xlabel(\"Epoch\")\n",
    "axes[0, 1].set_title(\"Welfare\")\n",
    "\n",
    "# Acceptance rate over time\n",
    "axes[1, 0].plot(df_baseline[\"epoch\"], df_baseline[\"acceptance_rate\"], \"^-\", color=\"#1f77b4\")\n",
    "axes[1, 0].set_ylabel(\"Acceptance rate\")\n",
    "axes[1, 0].set_xlabel(\"Epoch\")\n",
    "axes[1, 0].set_title(\"Acceptance Rate\")\n",
    "axes[1, 0].set_ylim(0, 1.05)\n",
    "\n",
    "# Per-agent payoff bar chart\n",
    "colors = [\"#2ca02c\" if t == \"honest\" else \"#ff7f0e\" if t == \"opportunistic\" else \"#d62728\"\n",
    "          for t in df_agents_baseline[\"type\"]]\n",
    "axes[1, 1].bar(df_agents_baseline[\"agent_id\"], df_agents_baseline[\"payoff\"], color=colors)\n",
    "axes[1, 1].set_ylabel(\"Total payoff\")\n",
    "axes[1, 1].set_title(\"Per-Agent Payoff\")\n",
    "axes[1, 1].tick_params(axis=\"x\", rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Add Adversaries\n\nNow let's run the **adversarial red-team** scenario: 8 agents (4 honest, 2 adversarial, 2 adaptive adversaries) with governance enabled (staking, circuit breakers, audits, collusion detection). The adversarial fraction is 50% — right at the critical threshold our research identified.\n\n**What to look for:** Compare the welfare and acceptance curves to the baseline above. Even with governance enabled, the system should collapse around epoch 12-14. This demonstrates a key finding: governance *delays* collapse but cannot prevent it when the adversarial fraction exceeds the critical threshold."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and run adversarial scenario\n",
    "adv_scenario = load_scenario(SCENARIOS_DIR / \"adversarial_redteam.yaml\")\n",
    "adv_orchestrator = build_orchestrator(adv_scenario)\n",
    "adv_history = adv_orchestrator.run()\n",
    "\n",
    "df_adv = pd.DataFrame([\n",
    "    {\n",
    "        \"epoch\": m.epoch,\n",
    "        \"toxicity\": m.toxicity_rate,\n",
    "        \"welfare\": m.total_welfare,\n",
    "        \"acceptance_rate\": m.accepted_interactions / max(m.total_interactions, 1),\n",
    "    }\n",
    "    for m in adv_history\n",
    "])\n",
    "\n",
    "# Overlay comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "fig.suptitle(\"Baseline vs Adversarial Red-Team\", fontsize=14)\n",
    "\n",
    "# Welfare comparison\n",
    "ax1.plot(df_baseline[\"epoch\"], df_baseline[\"welfare\"], \"s-\", label=\"Baseline (20% adv)\", color=\"#2ca02c\")\n",
    "ax1.plot(df_adv[\"epoch\"], df_adv[\"welfare\"], \"v-\", label=\"Red-Team (50% adv)\", color=\"#d62728\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Total welfare\")\n",
    "ax1.set_title(\"Welfare Collapse\")\n",
    "ax1.legend()\n",
    "ax1.axhline(y=0, color=\"gray\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Acceptance rate comparison\n",
    "ax2.plot(df_baseline[\"epoch\"], df_baseline[\"acceptance_rate\"], \"^-\", label=\"Baseline\", color=\"#1f77b4\")\n",
    "ax2.plot(df_adv[\"epoch\"], df_adv[\"acceptance_rate\"], \"v-\", label=\"Red-Team\", color=\"#d62728\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Acceptance rate\")\n",
    "ax2.set_title(\"Acceptance Rate Decline\")\n",
    "ax2.set_ylim(0, 1.05)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find collapse epoch (first epoch where welfare hits 0)\n",
    "collapse = df_adv[df_adv[\"welfare\"] <= 0.0]\n",
    "if not collapse.empty:\n",
    "    print(f\"\\nCollapse detected at epoch {collapse.iloc[0]['epoch']:.0f}\")\n",
    "    print(\"Governance delayed but did not prevent ecosystem failure.\")\n",
    "else:\n",
    "    print(\"\\nNo collapse detected -- governance held.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## What Just Happened?\n\nYou've just observed the **three regimes** that emerge from SWARM simulations:\n\n| Regime | Adversarial % | Acceptance | Toxicity | Outcome |\n|--------|--------------|------------|----------|---------|\n| **Cooperative** | 0-20% | > 0.93 | < 0.30 | Stable welfare growth |\n| **Contested** | 20-37.5% | 0.42-0.94 | 0.33-0.37 | Declining but survivable |\n| **Collapse** | 50% | < 0.56 | ~0.30 | Welfare hits zero by epoch 12-14 |\n\nKey findings from 11 scenario runs:\n- **Critical threshold** at 37.5-50% adversarial fraction separates survival from collapse\n- **Governance tuning** (audits, staking, circuit breakers) delays collapse by 2 epochs but doesn't prevent it\n- **Collusion detection** is the critical differentiator: at 37.5% adversarial, it prevents collapse entirely\n- **Welfare scales super-linearly** with cooperative agents (3 agents: 1.0, 6 agents: 5.7, 10 agents: 21.3)\n\n## Next Steps\n\n**Try these examples** (each runs standalone, no API keys needed):\n\n| Example | What it does | How to run |\n|---------|-------------|------------|\n| `illusion_delta_minimal.py` | Replay-based incoherence detection with 3 agents | `python examples/illusion_delta_minimal.py` |\n| `mvp_demo.py` | Full 5-agent simulation with metrics printout | `python examples/mvp_demo.py` |\n| `run_scenario.py` | Run any YAML scenario from the CLI | `python examples/run_scenario.py scenarios/baseline.yaml` |\n| `parameter_sweep.py` | Sweep governance parameters and compare outcomes | `python examples/parameter_sweep.py` |\n| `run_redteam.py` | Red-team evaluation across 8 attack vectors | `python examples/run_redteam.py --mode quick` |\n\n**Go deeper:**\n- Read the full paper: `docs/papers/distributional_agi_safety.md`\n- Write your own scenario YAML — see `scenarios/baseline.yaml` as a template\n- Explore 55 built-in scenarios: `swarm list`\n- Try LLM-backed agents (requires API key): `python examples/llm_demo.py`\n\n**Repository:** [github.com/swarm-ai-safety/swarm](https://github.com/swarm-ai-safety/swarm)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}