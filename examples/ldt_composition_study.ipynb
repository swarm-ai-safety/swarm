{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/swarm-ai-safety/swarm/blob/main/examples/ldt_composition_study.ipynb)\n",
    "\n",
    "# LDT Composition Study: Logical Decision Theory Agents\n",
    "\n",
    "This notebook compares **LDT (Logical Decision Theory)** agents against **honest** agents\n",
    "to determine whether LDT-style reasoning produces better welfare and toxicity outcomes\n",
    "across varying population compositions.\n",
    "\n",
    "We run two parallel sweeps:\n",
    "- **LDT sweep**: Vary LDT agents from 0% to 100%, filling remaining slots with deceptive (60%) + opportunistic (40%)\n",
    "- **Honest sweep**: Same structure but with honest agents as the focal type\n",
    "\n",
    "This allows direct comparison of how each agent type performs as its proportion in the population changes.\n",
    "\n",
    "**Difficulty:** Advanced\n",
    "\n",
    "**No API keys required. Runs entirely locally (or in Colab).**"
   ],
   "id": "e4494371"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "# This cell handles installation automatically.\n",
    "# In Colab: clones the repo and installs SWARM.\n",
    "# Locally: assumes you've already run `pip install -e \".[runtime]\"`.\n",
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    !git clone --depth 1 https://github.com/swarm-ai-safety/swarm.git /content/swarm\n",
    "    %pip install -q -e \"/content/swarm[runtime]\"\n",
    "    os.chdir(\"/content/swarm\")\n",
    "    print(\"Installed SWARM from GitHub. Ready to go!\")\n",
    "else:\n",
    "    print(\"Local environment detected -- using existing install.\")"
   ],
   "id": "64709737"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDT Agents and Study Methodology\n",
    "\n",
    "**Logical Decision Theory (LDT)** agents make decisions based on the logical consequences\n",
    "of their decision algorithm being a certain type. Unlike CDT (causal) or EDT (evidential)\n",
    "agents, LDT agents reason about what happens in worlds where agents with their source code\n",
    "make a given choice. This can lead to more cooperative behavior even without direct\n",
    "communication or repeated interaction.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- **Population size**: Fixed at a set number of agents per run\n",
    "- **Focal agent sweep**: Vary the focal agent type (LDT or honest) from 0% to 100% in 10% increments\n",
    "- **Background agents**: Remaining slots are filled with a mix of deceptive (60%) and opportunistic (40%) agents\n",
    "- **Two parallel sweeps**: One sweeping LDT agents, one sweeping honest agents, for direct comparison\n",
    "- **Payoff parameters**: Baseline settings (s_plus=2, s_minus=1, h=1)\n",
    "- **Multiple seeds**: Each configuration is run with multiple random seeds and results are averaged\n",
    "\n",
    "This design isolates the effect of agent decision-theoretic type on ecosystem outcomes."
   ],
   "id": "b0f35fba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "from swarm.agents.deceptive import DeceptiveAgent\n",
    "from swarm.agents.honest import HonestAgent\n",
    "from swarm.agents.ldt_agent import LDTAgent\n",
    "from swarm.agents.opportunistic import OpportunisticAgent\n",
    "from swarm.core.orchestrator import Orchestrator, OrchestratorConfig\n",
    "from swarm.core.payoff import PayoffConfig\n",
    "from swarm.governance.config import GovernanceConfig"
   ],
   "id": "2df20810"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Parameters ---\n",
    "# These are reduced for Colab speed. For a full study, increase to:\n",
    "#   TOTAL_AGENTS=10, N_EPOCHS=30, STEPS_PER_EPOCH=10, SEEDS=[42, 43, 44]\n",
    "TOTAL_AGENTS = 6\n",
    "N_EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 5\n",
    "SEEDS = [42, 43]\n",
    "\n",
    "\n",
    "def build_compositions(total_agents):\n",
    "    \"\"\"Build compositions for both LDT and honest sweeps.\n",
    "\n",
    "    Each sweep varies the focal agent type from 0% to 100% in 10% steps.\n",
    "    Remaining slots are filled with deceptive (60%) + opportunistic (40%).\n",
    "    \"\"\"\n",
    "    compositions = []\n",
    "    for sweep_type in (\"ldt\", \"honest\"):\n",
    "        for focal_pct in range(0, 101, 10):\n",
    "            n_focal = round(total_agents * focal_pct / 100)\n",
    "            remaining = total_agents - n_focal\n",
    "            n_deceptive = round(remaining * 0.6)\n",
    "            n_opportunistic = remaining - n_deceptive\n",
    "\n",
    "            if sweep_type == \"ldt\":\n",
    "                compositions.append({\n",
    "                    \"label\": f\"LDT {focal_pct}%\",\n",
    "                    \"sweep_type\": \"ldt\",\n",
    "                    \"n_ldt\": max(n_focal, 0),\n",
    "                    \"n_honest\": 0,\n",
    "                    \"n_deceptive\": max(n_deceptive, 0),\n",
    "                    \"n_opportunistic\": max(n_opportunistic, 0),\n",
    "                })\n",
    "            else:\n",
    "                compositions.append({\n",
    "                    \"label\": f\"Honest {focal_pct}%\",\n",
    "                    \"sweep_type\": \"honest\",\n",
    "                    \"n_ldt\": 0,\n",
    "                    \"n_honest\": max(n_focal, 0),\n",
    "                    \"n_deceptive\": max(n_deceptive, 0),\n",
    "                    \"n_opportunistic\": max(n_opportunistic, 0),\n",
    "                })\n",
    "    return compositions\n",
    "\n",
    "\n",
    "def create_agents_from_config(comp):\n",
    "    \"\"\"Create agent instances from a composition dict.\"\"\"\n",
    "    agents = []\n",
    "    for i in range(comp[\"n_ldt\"]):\n",
    "        agents.append(LDTAgent(agent_id=f\"ldt_{i + 1}\"))\n",
    "    for i in range(comp[\"n_honest\"]):\n",
    "        agents.append(HonestAgent(agent_id=f\"honest_{i + 1}\"))\n",
    "    for i in range(comp[\"n_deceptive\"]):\n",
    "        agents.append(DeceptiveAgent(agent_id=f\"deceptive_{i + 1}\"))\n",
    "    for i in range(comp[\"n_opportunistic\"]):\n",
    "        agents.append(OpportunisticAgent(agent_id=f\"opportunistic_{i + 1}\"))\n",
    "    return agents\n",
    "\n",
    "\n",
    "def _mean_or_zero(vals):\n",
    "    return float(np.mean(vals)) if vals else 0.0\n",
    "\n",
    "\n",
    "def run_single(comp, seed, n_epochs, steps_per_epoch):\n",
    "    \"\"\"Run one simulation for a given composition and seed.\"\"\"\n",
    "    payoff_config = PayoffConfig(\n",
    "        s_plus=2.0, s_minus=1.0, h=1.0,\n",
    "        theta=0.5, rho_a=0.1, rho_b=0.1, w_rep=0.1,\n",
    "    )\n",
    "    governance_config = GovernanceConfig(\n",
    "        circuit_breaker_enabled=True,\n",
    "        reputation_decay_rate=0.05,\n",
    "    )\n",
    "    orch_config = OrchestratorConfig(\n",
    "        n_epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        seed=seed,\n",
    "        payoff_config=payoff_config,\n",
    "        governance_config=governance_config,\n",
    "    )\n",
    "\n",
    "    orchestrator = Orchestrator(config=orch_config)\n",
    "    agents = create_agents_from_config(comp)\n",
    "    for agent in agents:\n",
    "        orchestrator.register_agent(agent)\n",
    "\n",
    "    epoch_metrics = orchestrator.run()\n",
    "\n",
    "    welfares = [em.total_welfare for em in epoch_metrics]\n",
    "    toxicities = [em.toxicity_rate for em in epoch_metrics]\n",
    "    qgaps = [em.quality_gap for em in epoch_metrics]\n",
    "    payoffs = [em.avg_payoff for em in epoch_metrics]\n",
    "\n",
    "    # Extract per-class payoffs\n",
    "    class_payoffs = {\"ldt\": [], \"honest\": [], \"deceptive\": [], \"opportunistic\": []}\n",
    "    for agent in orchestrator.get_all_agents():\n",
    "        state = orchestrator.state.get_agent(agent.agent_id)\n",
    "        if state is None:\n",
    "            continue\n",
    "        if isinstance(agent, LDTAgent):\n",
    "            class_payoffs[\"ldt\"].append(state.total_payoff)\n",
    "        elif isinstance(agent, HonestAgent):\n",
    "            class_payoffs[\"honest\"].append(state.total_payoff)\n",
    "        elif isinstance(agent, DeceptiveAgent):\n",
    "            class_payoffs[\"deceptive\"].append(state.total_payoff)\n",
    "        elif isinstance(agent, OpportunisticAgent):\n",
    "            class_payoffs[\"opportunistic\"].append(state.total_payoff)\n",
    "\n",
    "    total = comp[\"n_ldt\"] + comp[\"n_honest\"] + comp[\"n_deceptive\"] + comp[\"n_opportunistic\"]\n",
    "    if comp[\"sweep_type\"] == \"ldt\":\n",
    "        focal_pct = comp[\"n_ldt\"] / total if total else 0.0\n",
    "    else:\n",
    "        focal_pct = comp[\"n_honest\"] / total if total else 0.0\n",
    "\n",
    "    return {\n",
    "        \"composition\": comp[\"label\"],\n",
    "        \"sweep_type\": comp[\"sweep_type\"],\n",
    "        \"focal_pct\": focal_pct,\n",
    "        \"seed\": seed,\n",
    "        \"n_epochs\": len(epoch_metrics),\n",
    "        \"mean_welfare\": float(np.mean(welfares)) if welfares else 0.0,\n",
    "        \"total_welfare\": float(np.sum(welfares)) if welfares else 0.0,\n",
    "        \"mean_toxicity\": float(np.mean(toxicities)) if toxicities else 0.0,\n",
    "        \"mean_quality_gap\": float(np.mean(qgaps)) if qgaps else 0.0,\n",
    "        \"mean_avg_payoff\": float(np.mean(payoffs)) if payoffs else 0.0,\n",
    "        \"final_welfare\": float(welfares[-1]) if welfares else 0.0,\n",
    "        \"final_toxicity\": float(toxicities[-1]) if toxicities else 0.0,\n",
    "        \"ldt_avg_payoff\": _mean_or_zero(class_payoffs[\"ldt\"]),\n",
    "        \"honest_avg_payoff\": _mean_or_zero(class_payoffs[\"honest\"]),\n",
    "        \"deceptive_avg_payoff\": _mean_or_zero(class_payoffs[\"deceptive\"]),\n",
    "        \"opportunistic_avg_payoff\": _mean_or_zero(class_payoffs[\"opportunistic\"]),\n",
    "    }\n",
    "\n",
    "\n",
    "def aggregate_results(results):\n",
    "    \"\"\"Group by composition and compute mean/std across seeds.\"\"\"\n",
    "    groups = defaultdict(list)\n",
    "    for r in results:\n",
    "        groups[r[\"composition\"]].append(r)\n",
    "\n",
    "    aggs = []\n",
    "    for label, runs in sorted(groups.items(), key=lambda x: (x[1][0][\"sweep_type\"], x[1][0][\"focal_pct\"])):\n",
    "        welfares = [r[\"mean_welfare\"] for r in runs]\n",
    "        welfare_totals = [r[\"total_welfare\"] for r in runs]\n",
    "        toxicities = [r[\"mean_toxicity\"] for r in runs]\n",
    "        qgaps = [r[\"mean_quality_gap\"] for r in runs]\n",
    "        payoffs_all = [r[\"mean_avg_payoff\"] for r in runs]\n",
    "\n",
    "        aggs.append({\n",
    "            \"label\": label,\n",
    "            \"sweep_type\": runs[0][\"sweep_type\"],\n",
    "            \"focal_pct\": runs[0][\"focal_pct\"],\n",
    "            \"n_seeds\": len(runs),\n",
    "            \"welfare_mean\": float(np.mean(welfares)),\n",
    "            \"welfare_std\": float(np.std(welfares)),\n",
    "            \"welfare_total_mean\": float(np.mean(welfare_totals)),\n",
    "            \"toxicity_mean\": float(np.mean(toxicities)),\n",
    "            \"toxicity_std\": float(np.std(toxicities)),\n",
    "            \"quality_gap_mean\": float(np.mean(qgaps)),\n",
    "            \"avg_payoff_mean\": float(np.mean(payoffs_all)),\n",
    "            \"ldt_payoff_mean\": float(np.mean([r[\"ldt_avg_payoff\"] for r in runs])),\n",
    "            \"honest_payoff_mean\": float(np.mean([r[\"honest_avg_payoff\"] for r in runs])),\n",
    "            \"deceptive_payoff_mean\": float(np.mean([r[\"deceptive_avg_payoff\"] for r in runs])),\n",
    "            \"opportunistic_payoff_mean\": float(np.mean([r[\"opportunistic_avg_payoff\"] for r in runs])),\n",
    "        })\n",
    "    return aggs\n",
    "\n",
    "\n",
    "print(f\"Configuration: {TOTAL_AGENTS} agents, {N_EPOCHS} epochs, {STEPS_PER_EPOCH} steps/epoch, {len(SEEDS)} seeds\")"
   ],
   "id": "de11be2e"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Study\n",
    "\n",
    "We now run all compositions across both sweeps (LDT and honest) with multiple seeds.\n",
    "This produces 22 compositions x N seeds = total runs."
   ],
   "id": "a869d470"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compositions = build_compositions(TOTAL_AGENTS)\n",
    "all_results = []\n",
    "total_runs = len(compositions) * len(SEEDS)\n",
    "run_idx = 0\n",
    "\n",
    "print(f\"Running {total_runs} simulations ({len(compositions)} compositions x {len(SEEDS)} seeds)...\")\n",
    "print()\n",
    "\n",
    "for comp in compositions:\n",
    "    for seed in SEEDS:\n",
    "        run_idx += 1\n",
    "        print(\n",
    "            f\"  [{run_idx}/{total_runs}] {comp['label']} \"\n",
    "            f\"(L={comp['n_ldt']}, H={comp['n_honest']}, \"\n",
    "            f\"D={comp['n_deceptive']}, O={comp['n_opportunistic']}) \"\n",
    "            f\"seed={seed}...\",\n",
    "            end=\" \", flush=True,\n",
    "        )\n",
    "        result = run_single(comp, seed, N_EPOCHS, STEPS_PER_EPOCH)\n",
    "        all_results.append(result)\n",
    "        print(f\"welfare={result['total_welfare']:.1f}, toxicity={result['mean_toxicity']:.3f}\")\n",
    "\n",
    "print(f\"\\nCompleted {len(all_results)} runs.\")"
   ],
   "id": "274ae89d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results across seeds\n",
    "aggs = aggregate_results(all_results)\n",
    "\n",
    "# Display as a table\n",
    "df_aggs = pd.DataFrame(aggs)\n",
    "display_cols = [\n",
    "    \"label\", \"sweep_type\", \"focal_pct\", \"welfare_total_mean\", \"welfare_std\",\n",
    "    \"toxicity_mean\", \"toxicity_std\", \"ldt_payoff_mean\", \"honest_payoff_mean\",\n",
    "    \"deceptive_payoff_mean\", \"opportunistic_payoff_mean\",\n",
    "]\n",
    "print(\"=\" * 80)\n",
    "print(\"AGGREGATED RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(df_aggs[display_cols].to_string(index=False))\n",
    "\n",
    "# Key comparisons\n",
    "ldt_aggs = [a for a in aggs if a[\"sweep_type\"] == \"ldt\"]\n",
    "honest_aggs = [a for a in aggs if a[\"sweep_type\"] == \"honest\"]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY COMPARISONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if ldt_aggs:\n",
    "    ldt_peak = max(ldt_aggs, key=lambda a: a[\"welfare_total_mean\"])\n",
    "    print(f\"  LDT peak welfare: {ldt_peak['label']} \"\n",
    "          f\"(welfare={ldt_peak['welfare_total_mean']:.2f}, toxicity={ldt_peak['toxicity_mean']:.3f})\")\n",
    "if honest_aggs:\n",
    "    honest_peak = max(honest_aggs, key=lambda a: a[\"welfare_total_mean\"])\n",
    "    print(f\"  Honest peak welfare: {honest_peak['label']} \"\n",
    "          f\"(welfare={honest_peak['welfare_total_mean']:.2f}, toxicity={honest_peak['toxicity_mean']:.3f})\")\n",
    "\n",
    "for pct in [0.2, 0.5, 1.0]:\n",
    "    ldt_match = next((a for a in ldt_aggs if round(a[\"focal_pct\"], 1) == pct), None)\n",
    "    hon_match = next((a for a in honest_aggs if round(a[\"focal_pct\"], 1) == pct), None)\n",
    "    if ldt_match and hon_match:\n",
    "        w_diff = ldt_match[\"welfare_total_mean\"] - hon_match[\"welfare_total_mean\"]\n",
    "        t_diff = ldt_match[\"toxicity_mean\"] - hon_match[\"toxicity_mean\"]\n",
    "        print(f\"  At {pct*100:.0f}%: LDT welfare={ldt_match['welfare_total_mean']:.2f} vs \"\n",
    "              f\"Honest welfare={hon_match['welfare_total_mean']:.2f} \"\n",
    "              f\"(diff={w_diff:+.2f}), toxicity diff={t_diff:+.3f}\")"
   ],
   "id": "54d04183"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 1: LDT vs Honest Welfare Comparison ---\n",
    "\n",
    "COLORS = {\n",
    "    \"ldt\": \"#9C27B0\",\n",
    "    \"honest\": \"#2196F3\",\n",
    "    \"deceptive\": \"#F44336\",\n",
    "    \"opportunistic\": \"#FF9800\",\n",
    "}\n",
    "\n",
    "ldt_sorted = sorted([a for a in aggs if a[\"sweep_type\"] == \"ldt\"], key=lambda a: a[\"focal_pct\"])\n",
    "hon_sorted = sorted([a for a in aggs if a[\"sweep_type\"] == \"honest\"], key=lambda a: a[\"focal_pct\"])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# LDT sweep\n",
    "ldt_pcts = [a[\"focal_pct\"] * 100 for a in ldt_sorted]\n",
    "ldt_welfares = [a[\"welfare_total_mean\"] for a in ldt_sorted]\n",
    "ldt_stds = [a[\"welfare_std\"] * a[\"n_seeds\"] for a in ldt_sorted]\n",
    "ax.errorbar(\n",
    "    ldt_pcts, ldt_welfares, yerr=ldt_stds,\n",
    "    color=COLORS[\"ldt\"], linewidth=2.5, marker=\"D\", markersize=8,\n",
    "    capsize=5, capthick=1.5, label=\"LDT agents\", zorder=5,\n",
    ")\n",
    "\n",
    "# Honest sweep\n",
    "hon_pcts = [a[\"focal_pct\"] * 100 for a in hon_sorted]\n",
    "hon_welfares = [a[\"welfare_total_mean\"] for a in hon_sorted]\n",
    "hon_stds = [a[\"welfare_std\"] * a[\"n_seeds\"] for a in hon_sorted]\n",
    "ax.errorbar(\n",
    "    hon_pcts, hon_welfares, yerr=hon_stds,\n",
    "    color=COLORS[\"honest\"], linewidth=2.5, marker=\"o\", markersize=8,\n",
    "    capsize=5, capthick=1.5, label=\"Honest agents\", zorder=4,\n",
    ")\n",
    "\n",
    "ax.set_title(\"LDT vs Honest: Total Welfare by Focal Agent Proportion\", fontsize=13, fontweight=\"bold\", pad=10)\n",
    "ax.set_xlabel(\"Focal Agent Proportion (%)\", fontsize=11)\n",
    "ax.set_ylabel(\"Total Welfare (sum over epochs)\", fontsize=11)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.legend(fontsize=10, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1ff87fc6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 2: LDT vs Honest Toxicity Comparison ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# LDT sweep\n",
    "ldt_tox = [a[\"toxicity_mean\"] for a in ldt_sorted]\n",
    "ldt_tox_stds = [a[\"toxicity_std\"] for a in ldt_sorted]\n",
    "ax.errorbar(\n",
    "    ldt_pcts, ldt_tox, yerr=ldt_tox_stds,\n",
    "    color=COLORS[\"ldt\"], linewidth=2.5, marker=\"D\", markersize=8,\n",
    "    capsize=5, capthick=1.5, label=\"LDT agents\", zorder=5,\n",
    ")\n",
    "\n",
    "# Honest sweep\n",
    "hon_tox = [a[\"toxicity_mean\"] for a in hon_sorted]\n",
    "hon_tox_stds = [a[\"toxicity_std\"] for a in hon_sorted]\n",
    "ax.errorbar(\n",
    "    hon_pcts, hon_tox, yerr=hon_tox_stds,\n",
    "    color=COLORS[\"honest\"], linewidth=2.5, marker=\"o\", markersize=8,\n",
    "    capsize=5, capthick=1.5, label=\"Honest agents\", zorder=4,\n",
    ")\n",
    "\n",
    "ax.set_title(\"LDT vs Honest: Toxicity Rate by Focal Agent Proportion\", fontsize=13, fontweight=\"bold\", pad=10)\n",
    "ax.set_xlabel(\"Focal Agent Proportion (%)\", fontsize=11)\n",
    "ax.set_ylabel(\"Toxicity Rate (mean over epochs)\", fontsize=11)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.legend(fontsize=10, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "f4f2d152"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 3: Welfare-Toxicity Scatter ---\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 7))\n",
    "\n",
    "for a in aggs:\n",
    "    pct = a[\"focal_pct\"] * 100\n",
    "    if a[\"sweep_type\"] == \"ldt\":\n",
    "        color = COLORS[\"ldt\"]\n",
    "        marker = \"D\"\n",
    "    else:\n",
    "        color = COLORS[\"honest\"]\n",
    "        marker = \"o\"\n",
    "\n",
    "    ax.scatter(\n",
    "        a[\"toxicity_mean\"], a[\"welfare_total_mean\"],\n",
    "        c=color, s=120, marker=marker, alpha=0.85,\n",
    "        edgecolors=\"white\", linewidth=1, zorder=5,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"{pct:.0f}%\",\n",
    "        (a[\"toxicity_mean\"], a[\"welfare_total_mean\"]),\n",
    "        textcoords=\"offset points\", xytext=(8, 5), fontsize=8,\n",
    "    )\n",
    "\n",
    "# Legend entries\n",
    "ax.scatter([], [], c=COLORS[\"ldt\"], s=100, marker=\"D\", label=\"LDT sweep\")\n",
    "ax.scatter([], [], c=COLORS[\"honest\"], s=100, marker=\"o\", label=\"Honest sweep\")\n",
    "\n",
    "ax.set_title(\"Welfare-Toxicity Trade-off: LDT vs Honest Compositions\", fontsize=13, fontweight=\"bold\", pad=10)\n",
    "ax.set_xlabel(\"Toxicity Rate\", fontsize=11)\n",
    "ax.set_ylabel(\"Total Welfare\", fontsize=11)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "ax.legend(fontsize=10, loc=\"best\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "3b0c6a4c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot 4: Per-Class Payoff Breakdown at Key Compositions ---\n",
    "\n",
    "key_pcts = [0.0, 0.2, 0.5, 0.8, 1.0]\n",
    "\n",
    "ldt_selected = [a for a in ldt_sorted if round(a[\"focal_pct\"], 1) in key_pcts]\n",
    "hon_selected = [a for a in hon_sorted if round(a[\"focal_pct\"], 1) in key_pcts]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n",
    "\n",
    "for ax, selected, title_prefix in [\n",
    "    (axes[0], ldt_selected, \"LDT Sweep\"),\n",
    "    (axes[1], hon_selected, \"Honest Sweep\"),\n",
    "]:\n",
    "    if not selected:\n",
    "        continue\n",
    "    labels = [a[\"label\"] for a in selected]\n",
    "    x = np.arange(len(labels))\n",
    "    width = 0.18\n",
    "\n",
    "    bars_data = [\n",
    "        (\"LDT\", [a[\"ldt_payoff_mean\"] for a in selected], COLORS[\"ldt\"]),\n",
    "        (\"Honest\", [a[\"honest_payoff_mean\"] for a in selected], COLORS[\"honest\"]),\n",
    "        (\"Deceptive\", [a[\"deceptive_payoff_mean\"] for a in selected], COLORS[\"deceptive\"]),\n",
    "        (\"Opportunistic\", [a[\"opportunistic_payoff_mean\"] for a in selected], COLORS[\"opportunistic\"]),\n",
    "    ]\n",
    "\n",
    "    for idx, (name, vals, color) in enumerate(bars_data):\n",
    "        offset = (idx - 1.5) * width\n",
    "        bars = ax.bar(x + offset, vals, width, label=name, color=color, alpha=0.85)\n",
    "        for bar, val in zip(bars, vals):\n",
    "            if val != 0.0:\n",
    "                ax.text(\n",
    "                    bar.get_x() + bar.get_width() / 2,\n",
    "                    bar.get_height() + 0.2,\n",
    "                    f\"{val:.1f}\",\n",
    "                    ha=\"center\", va=\"bottom\", fontsize=7, fontweight=\"bold\",\n",
    "                )\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(labels, fontsize=9, rotation=15, ha=\"right\")\n",
    "    ax.set_title(f\"{title_prefix}: Per-Class Payoffs\", fontsize=12, fontweight=\"bold\")\n",
    "    if ax == axes[0]:\n",
    "        ax.set_ylabel(\"Average Total Payoff\", fontsize=11)\n",
    "    ax.legend(fontsize=8, loc=\"best\")\n",
    "    ax.grid(True, alpha=0.3, linestyle=\"--\", axis=\"y\")\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "fig.suptitle(\n",
    "    \"Per-Class Payoff Breakdown at Key Compositions\",\n",
    "    fontsize=14, fontweight=\"bold\", y=1.02,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5c30ac37"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings and Next Steps\n",
    "\n",
    "### What to look for in the results\n",
    "\n",
    "- **Welfare curves**: Does the LDT sweep produce higher total welfare than the honest sweep at equivalent focal agent proportions? LDT agents reason about logical consequences of their decision algorithm, which may lead to more cooperative equilibria.\n",
    "- **Toxicity curves**: Does LDT reasoning reduce ecosystem toxicity more effectively than honest behavior?\n",
    "- **Welfare-toxicity trade-off**: Which sweep achieves better Pareto-optimal points (high welfare, low toxicity)?\n",
    "- **Per-class payoffs**: Do LDT agents earn more than honest agents? Do they also reduce deceptive/opportunistic agent payoffs (indicating better resistance to exploitation)?\n",
    "- **0% sanity check**: At 0% focal agents, both sweeps should produce identical results (pure deceptive + opportunistic population).\n",
    "\n",
    "### Comparisons\n",
    "\n",
    "| Metric | LDT Advantage? | Why |\n",
    "|--------|---------------|-----|\n",
    "| Welfare | Check curves | LDT may coordinate better via logical reasoning |\n",
    "| Toxicity | Check curves | LDT may avoid harmful interactions more reliably |\n",
    "| Exploitation resistance | Check payoff breakdown | LDT may be harder for deceptive agents to exploit |\n",
    "\n",
    "### Next steps\n",
    "\n",
    "- **Increase parameters**: For publication-quality results, use `TOTAL_AGENTS=10`, `N_EPOCHS=30`, `STEPS_PER_EPOCH=10`, `SEEDS=[42, 43, 44]`\n",
    "- **Run the full script**: `python examples/ldt_composition_study.py` for full-scale results with CSV export and saved plots\n",
    "- **Try other agent types**: Swap LDT for other decision-theoretic types to compare CDT, EDT, etc.\n",
    "- **Add governance**: Vary governance parameters (staking, audits) to see how LDT agents interact with institutional mechanisms\n",
    "\n",
    "**Repository:** [github.com/swarm-ai-safety/swarm](https://github.com/swarm-ai-safety/swarm)"
   ],
   "id": "0fad7655"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 0,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
