{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/swarm-ai-safety/swarm/blob/main/examples/reproduce_2602_00035.ipynb)\n",
    "\n",
    "# Reproduce Paper Results: Welfare vs. Honest Agent Proportion\n",
    "\n",
    "This notebook reproduces the central finding from **agentxiv paper 2602.00035**: that population composition has a counter-intuitive, non-monotonic effect on ecosystem welfare.\n",
    "\n",
    "We sweep the fraction of honest agents from 0% to 100% and measure welfare and toxicity at each point, replicating the paper's methodology using the SWARM simulation framework.\n",
    "\n",
    "**Difficulty:** Advanced\n",
    "\n",
    "**No API keys required. Runs entirely locally (or in Colab).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "# This cell handles installation automatically.\n",
    "# In Colab: clones the repo and installs SWARM.\n",
    "# Locally: assumes you've already run `pip install -e \".[runtime]\"`.\n",
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    !git clone --depth 1 https://github.com/swarm-ai-safety/swarm.git /content/swarm\n",
    "    %pip install -q -e \"/content/swarm[runtime]\"\n",
    "    os.chdir(\"/content/swarm\")\n",
    "    print(\"Installed SWARM from GitHub. Ready to go!\")\n",
    "else:\n",
    "    print(\"Local environment detected â€” using existing install.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Paper Finding\n",
    "\n",
    "Paper 2602.00035 reports a surprising result:\n",
    "\n",
    "> **Populations with only 20% honest agents achieve 55% higher welfare (53.67) than 100% honest populations (34.71), despite having significantly higher toxicity (0.344 vs 0.254).**\n",
    "\n",
    "This is counter-intuitive: adding more honest agents does *not* monotonically improve ecosystem welfare. The reason is that mixed populations generate more diverse interaction patterns. Opportunistic and deceptive agents create competitive pressure that, at moderate levels, drives higher total surplus --- even though some of that surplus comes at the cost of increased toxicity.\n",
    "\n",
    "### Methodology\n",
    "\n",
    "- Fix total population at **10 agents**\n",
    "- Vary honest agent proportion from **0% to 100%** in 10% steps\n",
    "- Fill non-honest slots with a **60/40 mix** of deceptive and opportunistic agents\n",
    "- Use baseline payoff parameters (`s_plus=2`, `s_minus=1`, `h=1`)\n",
    "- Run each configuration for multiple epochs across multiple seeds\n",
    "- Compare welfare and toxicity across compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from swarm.agents.deceptive import DeceptiveAgent\n",
    "from swarm.agents.honest import HonestAgent\n",
    "from swarm.agents.opportunistic import OpportunisticAgent\n",
    "from swarm.core.orchestrator import Orchestrator, OrchestratorConfig\n",
    "from swarm.core.payoff import PayoffConfig\n",
    "from swarm.governance.config import GovernanceConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Experiment Parameters ---\n",
    "# Reduced for Colab speed. Increase for full reproduction:\n",
    "#   TOTAL_AGENTS = 10, N_EPOCHS = 30, STEPS_PER_EPOCH = 10, SEEDS = [42, 43, 44]\n",
    "TOTAL_AGENTS = 10\n",
    "N_EPOCHS = 10\n",
    "STEPS_PER_EPOCH = 10\n",
    "SEEDS = [42, 43]\n",
    "\n",
    "\n",
    "def build_compositions(total_agents):\n",
    "    \"\"\"Build compositions varying honest proportion from 0% to 100%.\"\"\"\n",
    "    compositions = []\n",
    "    for honest_pct in range(0, 101, 10):\n",
    "        n_honest = round(total_agents * honest_pct / 100)\n",
    "        remaining = total_agents - n_honest\n",
    "        # Split remaining 60/40 between deceptive and opportunistic\n",
    "        n_deceptive = round(remaining * 0.6)\n",
    "        n_opportunistic = remaining - n_deceptive\n",
    "        compositions.append({\n",
    "            \"label\": f\"{honest_pct}% honest\",\n",
    "            \"honest_pct\": honest_pct / 100.0,\n",
    "            \"n_honest\": max(n_honest, 0),\n",
    "            \"n_deceptive\": max(n_deceptive, 0),\n",
    "            \"n_opportunistic\": max(n_opportunistic, 0),\n",
    "        })\n",
    "    return compositions\n",
    "\n",
    "\n",
    "def run_single(comp, seed, n_epochs, steps_per_epoch):\n",
    "    \"\"\"Run one simulation for a given composition and seed.\"\"\"\n",
    "    payoff_config = PayoffConfig(\n",
    "        s_plus=2.0, s_minus=1.0, h=1.0,\n",
    "        theta=0.5, rho_a=0.1, rho_b=0.1, w_rep=0.1,\n",
    "    )\n",
    "    governance_config = GovernanceConfig(\n",
    "        circuit_breaker_enabled=True,\n",
    "        reputation_decay_rate=0.05,\n",
    "    )\n",
    "    orch_config = OrchestratorConfig(\n",
    "        n_epochs=n_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        seed=seed,\n",
    "        payoff_config=payoff_config,\n",
    "        governance_config=governance_config,\n",
    "    )\n",
    "\n",
    "    orchestrator = Orchestrator(config=orch_config)\n",
    "    # Create agents\n",
    "    agents = []\n",
    "    for i in range(comp[\"n_honest\"]):\n",
    "        agents.append(HonestAgent(agent_id=f\"honest_{i + 1}\"))\n",
    "    for i in range(comp[\"n_deceptive\"]):\n",
    "        agents.append(DeceptiveAgent(agent_id=f\"deceptive_{i + 1}\"))\n",
    "    for i in range(comp[\"n_opportunistic\"]):\n",
    "        agents.append(OpportunisticAgent(agent_id=f\"opportunistic_{i + 1}\"))\n",
    "    for agent in agents:\n",
    "        orchestrator.register_agent(agent)\n",
    "\n",
    "    epoch_metrics = orchestrator.run()\n",
    "\n",
    "    welfares = [em.total_welfare for em in epoch_metrics]\n",
    "    toxicities = [em.toxicity_rate for em in epoch_metrics]\n",
    "    qgaps = [em.quality_gap for em in epoch_metrics]\n",
    "    payoffs = [em.avg_payoff for em in epoch_metrics]\n",
    "\n",
    "    return {\n",
    "        \"composition\": comp[\"label\"],\n",
    "        \"honest_pct\": comp[\"honest_pct\"],\n",
    "        \"seed\": seed,\n",
    "        \"n_epochs\": len(epoch_metrics),\n",
    "        \"mean_welfare\": float(np.mean(welfares)) if welfares else 0.0,\n",
    "        \"total_welfare\": float(np.sum(welfares)) if welfares else 0.0,\n",
    "        \"mean_toxicity\": float(np.mean(toxicities)) if toxicities else 0.0,\n",
    "        \"mean_quality_gap\": float(np.mean(qgaps)) if qgaps else 0.0,\n",
    "        \"mean_avg_payoff\": float(np.mean(payoffs)) if payoffs else 0.0,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Sweep\n",
    "\n",
    "We sweep honest agent proportion from **0% to 100%** in 10% increments (11 compositions total). Each composition is run across multiple seeds to reduce variance. The non-honest slots are filled with a 60/40 split of deceptive and opportunistic agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compositions = build_compositions(TOTAL_AGENTS)\n",
    "all_results = []\n",
    "total_runs = len(compositions) * len(SEEDS)\n",
    "run_idx = 0\n",
    "\n",
    "print(f\"Running {total_runs} simulations ({len(compositions)} compositions x {len(SEEDS)} seeds)...\")\n",
    "print(f\"Parameters: {TOTAL_AGENTS} agents, {N_EPOCHS} epochs, {STEPS_PER_EPOCH} steps/epoch\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for comp in compositions:\n",
    "    for seed in SEEDS:\n",
    "        run_idx += 1\n",
    "        print(\n",
    "            f\"  [{run_idx}/{total_runs}] {comp['label']} \"\n",
    "            f\"(H={comp['n_honest']}, D={comp['n_deceptive']}, O={comp['n_opportunistic']}) \"\n",
    "            f\"seed={seed}...\",\n",
    "            end=\" \", flush=True,\n",
    "        )\n",
    "        result = run_single(comp, seed, N_EPOCHS, STEPS_PER_EPOCH)\n",
    "        all_results.append(result)\n",
    "        print(f\"welfare={result['total_welfare']:.1f}, toxicity={result['mean_toxicity']:.3f}\")\n",
    "\n",
    "print(\"\\nSweep complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results across seeds\n",
    "df_raw = pd.DataFrame(all_results)\n",
    "\n",
    "df_agg = df_raw.groupby(\"composition\").agg(\n",
    "    honest_pct=(\"honest_pct\", \"first\"),\n",
    "    n_seeds=(\"seed\", \"count\"),\n",
    "    welfare_total_mean=(\"total_welfare\", \"mean\"),\n",
    "    welfare_std=(\"mean_welfare\", \"std\"),\n",
    "    toxicity_mean=(\"mean_toxicity\", \"mean\"),\n",
    "    toxicity_std=(\"mean_toxicity\", \"std\"),\n",
    "    quality_gap_mean=(\"mean_quality_gap\", \"mean\"),\n",
    "    avg_payoff_mean=(\"mean_avg_payoff\", \"mean\"),\n",
    ").reset_index().sort_values(\"honest_pct\")\n",
    "\n",
    "df_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results table\n",
    "print(\"=\" * 90)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\" * 90)\n",
    "print(\n",
    "    f\"{'Composition':<18} {'Honest%':>8} {'TotalWelfare':>13} \"\n",
    "    f\"{'Toxicity':>9} {'QualGap':>8} {'AvgPayoff':>10}\"\n",
    ")\n",
    "print(\"-\" * 75)\n",
    "for _, row in df_agg.iterrows():\n",
    "    print(\n",
    "        f\"{row['composition']:<18} {row['honest_pct']*100:>7.0f}% \"\n",
    "        f\"{row['welfare_total_mean']:>13.2f} {row['toxicity_mean']:>9.3f} \"\n",
    "        f\"{row['quality_gap_mean']:>8.3f} {row['avg_payoff_mean']:>10.3f}\"\n",
    "    )\n",
    "\n",
    "# Key comparison: 20% vs 100% honest\n",
    "row_20 = df_agg[df_agg[\"honest_pct\"].round(1) == 0.2]\n",
    "row_100 = df_agg[df_agg[\"honest_pct\"].round(1) == 1.0]\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"KEY COMPARISON: 20% honest vs 100% honest\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not row_20.empty and not row_100.empty:\n",
    "    w20 = row_20.iloc[0][\"welfare_total_mean\"]\n",
    "    w100 = row_100.iloc[0][\"welfare_total_mean\"]\n",
    "    t20 = row_20.iloc[0][\"toxicity_mean\"]\n",
    "    t100 = row_100.iloc[0][\"toxicity_mean\"]\n",
    "    welfare_pct = ((w20 - w100) / w100 * 100) if w100 != 0 else float(\"inf\")\n",
    "\n",
    "    print(f\"  20% honest:  welfare={w20:.2f}, toxicity={t20:.3f}\")\n",
    "    print(f\"  100% honest: welfare={w100:.2f}, toxicity={t100:.3f}\")\n",
    "    print(f\"  Welfare difference: {welfare_pct:+.1f}%\")\n",
    "    print(f\"  Toxicity difference: {t20 - t100:+.3f}\")\n",
    "    print()\n",
    "    if welfare_pct > 0:\n",
    "        print(f\"  FINDING REPRODUCED: 20% honest has {welfare_pct:.0f}% higher welfare\")\n",
    "    else:\n",
    "        print(f\"  FINDING NOT REPRODUCED: 20% honest has lower welfare\")\n",
    "    print(f\"  Paper claims: 55% higher welfare (53.67 vs 34.71)\")\n",
    "    print(f\"  Our result:   {welfare_pct:.0f}% difference ({w20:.2f} vs {w100:.2f})\")\n",
    "\n",
    "    # Peak welfare\n",
    "    peak_idx = df_agg[\"welfare_total_mean\"].idxmax()\n",
    "    peak = df_agg.loc[peak_idx]\n",
    "    print(\n",
    "        f\"\\n  Peak welfare composition: {peak['composition']} \"\n",
    "        f\"(welfare={peak['welfare_total_mean']:.2f}, toxicity={peak['toxicity_mean']:.3f})\"\n",
    "    )\n",
    "else:\n",
    "    print(\"  Could not find both 20% and 100% compositions in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 1: Dual-axis welfare/toxicity vs honest %\n",
    "COLORS = {\"welfare\": \"#2196F3\", \"toxicity\": \"#F44336\", \"quality_gap\": \"#4CAF50\"}\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "pcts = df_agg[\"honest_pct\"] * 100\n",
    "welfares = df_agg[\"welfare_total_mean\"]\n",
    "welfare_stds = df_agg[\"welfare_std\"] * df_agg[\"n_seeds\"]\n",
    "toxicities = df_agg[\"toxicity_mean\"]\n",
    "toxicity_stds = df_agg[\"toxicity_std\"]\n",
    "\n",
    "# Welfare (left axis)\n",
    "ax1.errorbar(\n",
    "    pcts, welfares, yerr=welfare_stds,\n",
    "    color=COLORS[\"welfare\"], linewidth=2.5, marker=\"o\", markersize=8,\n",
    "    capsize=5, capthick=1.5, label=\"Total Welfare\", zorder=5,\n",
    ")\n",
    "ax1.set_ylabel(\"Total Welfare (sum over epochs)\", fontsize=11, color=COLORS[\"welfare\"])\n",
    "ax1.tick_params(axis=\"y\", labelcolor=COLORS[\"welfare\"])\n",
    "\n",
    "# Toxicity (right axis)\n",
    "ax2 = ax1.twinx()\n",
    "ax2.errorbar(\n",
    "    pcts, toxicities, yerr=toxicity_stds,\n",
    "    color=COLORS[\"toxicity\"], linewidth=2.5, marker=\"s\", markersize=8,\n",
    "    capsize=5, capthick=1.5, label=\"Toxicity Rate\", zorder=4,\n",
    ")\n",
    "ax2.set_ylabel(\"Toxicity Rate (mean over epochs)\", fontsize=11, color=COLORS[\"toxicity\"])\n",
    "ax2.tick_params(axis=\"y\", labelcolor=COLORS[\"toxicity\"])\n",
    "ax2.set_ylim(-0.05, 1.05)\n",
    "\n",
    "ax1.set_xlabel(\"Honest Agent Proportion (%)\", fontsize=11)\n",
    "ax1.set_title(\n",
    "    \"Reproduction of agentxiv 2602.00035:\\nWelfare vs. Toxicity by Honest Agent Proportion\",\n",
    "    fontsize=13, fontweight=\"bold\", pad=10,\n",
    ")\n",
    "ax1.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax1.spines[\"top\"].set_visible(False)\n",
    "\n",
    "# Combined legend\n",
    "lines1, labels1 = ax1.get_legend_handles_labels()\n",
    "lines2, labels2 = ax2.get_legend_handles_labels()\n",
    "ax1.legend(lines1 + lines2, labels1 + labels2, fontsize=10, loc=\"upper center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Welfare-toxicity scatter with composition labels\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "for _, row in df_agg.iterrows():\n",
    "    pct = row[\"honest_pct\"] * 100\n",
    "    r = max(0, 1.0 - row[\"honest_pct\"])\n",
    "    g = row[\"honest_pct\"]\n",
    "    b = 0.2\n",
    "    ax.scatter(\n",
    "        row[\"welfare_total_mean\"], row[\"toxicity_mean\"],\n",
    "        c=[(r, g, b)], s=120, edgecolors=\"white\", linewidth=1, zorder=5,\n",
    "    )\n",
    "    ax.annotate(\n",
    "        f\"{pct:.0f}%\",\n",
    "        (row[\"welfare_total_mean\"], row[\"toxicity_mean\"]),\n",
    "        textcoords=\"offset points\", xytext=(8, 5), fontsize=8,\n",
    "    )\n",
    "\n",
    "ax.set_title(\n",
    "    \"Welfare-Toxicity Trade-off by Honest Agent %\\n(Reproduction of 2602.00035)\",\n",
    "    fontsize=13, fontweight=\"bold\", pad=10,\n",
    ")\n",
    "ax.set_xlabel(\"Total Welfare\", fontsize=11)\n",
    "ax.set_ylabel(\"Toxicity Rate\", fontsize=11)\n",
    "ax.set_ylim(-0.05, 1.05)\n",
    "ax.grid(True, alpha=0.3, linestyle=\"--\")\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 3: Key compositions bar chart\n",
    "key_pcts = {0.0, 0.2, 0.5, 0.8, 1.0}\n",
    "df_key = df_agg[df_agg[\"honest_pct\"].round(1).isin(key_pcts)].copy()\n",
    "\n",
    "labels = df_key[\"composition\"].tolist()\n",
    "welfares = df_key[\"welfare_total_mean\"].tolist()\n",
    "toxicities = df_key[\"toxicity_mean\"].tolist()\n",
    "\n",
    "x = np.arange(len(labels))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "bars1 = ax1.bar(\n",
    "    x - width / 2, welfares, width,\n",
    "    label=\"Total Welfare\", color=COLORS[\"welfare\"], alpha=0.85,\n",
    ")\n",
    "ax1.set_ylabel(\"Total Welfare\", fontsize=11, color=COLORS[\"welfare\"])\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "bars2 = ax2.bar(\n",
    "    x + width / 2, toxicities, width,\n",
    "    label=\"Toxicity Rate\", color=COLORS[\"toxicity\"], alpha=0.85,\n",
    ")\n",
    "ax2.set_ylabel(\"Toxicity Rate\", fontsize=11, color=COLORS[\"toxicity\"])\n",
    "ax2.set_ylim(0, 1.0)\n",
    "\n",
    "# Value labels on bars\n",
    "for bar, val in zip(bars1, welfares):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.5,\n",
    "        f\"{val:.1f}\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\",\n",
    "    )\n",
    "for bar, val in zip(bars2, toxicities):\n",
    "    ax2.text(\n",
    "        bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,\n",
    "        f\"{val:.3f}\", ha=\"center\", va=\"bottom\", fontsize=9, fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(labels, fontsize=10)\n",
    "ax1.set_title(\n",
    "    \"Key Compositions: Welfare & Toxicity\\n(Reproduction of 2602.00035)\",\n",
    "    fontsize=13, fontweight=\"bold\",\n",
    ")\n",
    "ax1.legend(loc=\"upper left\", fontsize=9)\n",
    "ax2.legend(loc=\"upper right\", fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "### How Results Compare to the Paper\n",
    "\n",
    "Paper 2602.00035 reports that 20% honest populations achieve **55% higher welfare** (53.67 vs 34.71) than fully honest populations, with a toxicity increase from 0.254 to 0.344. The exact numbers from our reproduction will vary depending on the number of epochs and seeds used (this notebook uses reduced parameters for Colab speed), but the qualitative finding --- that **mixed populations outperform homogeneous honest populations on welfare** --- should be consistent.\n",
    "\n",
    "To reproduce the paper's exact numbers, increase the parameters at the top of cell 4:\n",
    "```python\n",
    "N_EPOCHS = 30\n",
    "SEEDS = [42, 43, 44]\n",
    "```\n",
    "\n",
    "### What This Means for AI Safety\n",
    "\n",
    "This finding has important implications for multi-agent AI system design:\n",
    "\n",
    "1. **Welfare is non-monotonic in honesty.** Naively maximizing the fraction of \"aligned\" agents does not maximize ecosystem welfare. This challenges simple alignment strategies that assume more aligned agents always produce better outcomes.\n",
    "\n",
    "2. **The welfare-toxicity trade-off is real.** Higher welfare comes at the cost of higher toxicity. System designers must decide which metric to optimize --- and that decision is fundamentally a values question, not a technical one.\n",
    "\n",
    "3. **Diversity creates value.** The competitive pressure from non-honest agents drives more interactions and higher total surplus. This mirrors findings in economics about the role of competition in generating welfare, even when individual actors are self-interested.\n",
    "\n",
    "4. **Governance design matters.** Rather than trying to eliminate all non-honest agents, the more productive approach may be to design governance mechanisms (reputation systems, circuit breakers, audits) that capture the welfare benefits of diversity while mitigating toxicity.\n",
    "\n",
    "For more details, see the full paper and the `examples/parameter_sweep.py` script for exploring governance configurations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}