{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/swarm-ai-safety/swarm/blob/main/examples/governance_mvp_sweep.ipynb)\n",
    "\n",
    "# Governance MVP Sweep: 12 Mechanism Comparison\n",
    "\n",
    "This notebook runs a **12-configuration governance sweep**, isolating each governance mechanism against a shared baseline to measure its effect on toxicity, welfare, quality gap, and per-agent-type payoffs.\n",
    "\n",
    "Each run enables a single governance lever (or, in the final run, a layered combination) so you can directly compare how each mechanism shapes ecosystem outcomes.\n",
    "\n",
    "**No API keys required. Runs entirely locally (or in Colab).**\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Difficulty** | Intermediate |\n",
    "| **Runtime** | ~2-5 minutes (Colab) |\n",
    "| **Prerequisites** | Familiarity with the quickstart notebook |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "# This cell handles installation automatically.\n",
    "# In Colab: clones the repo and installs SWARM.\n",
    "# Locally: assumes you've already run `pip install -e \".[runtime]\"`.\n",
    "import os\n",
    "\n",
    "if os.getenv(\"COLAB_RELEASE_TAG\"):\n",
    "    !git clone --depth 1 https://github.com/swarm-ai-safety/swarm.git /content/swarm\n",
    "    %pip install -q -e \"/content/swarm[runtime]\"\n",
    "    os.chdir(\"/content/swarm\")\n",
    "    print(\"Installed SWARM from GitHub. Ready to go!\")\n",
    "else:\n",
    "    print(\"Local environment detected \\u2014 using existing install.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## The 12 Governance Mechanisms\n",
    "\n",
    "| # | Mechanism | What it does |\n",
    "|---|---|---|\n",
    "| 1 | **Baseline (none)** | No governance â€” the control group |\n",
    "| 2 | **Reputation decay** | Reputations decay 10% per epoch, forcing agents to continuously earn trust |\n",
    "| 3 | **Vote normalization** | Caps vote influence per agent to prevent concentration of power |\n",
    "| 4 | **Bandwidth caps** | Limits interactions per step, throttling high-volume exploiters |\n",
    "| 5 | **Transparency ledger** | Public visibility of interaction quality with bonus for good actors |\n",
    "| 6 | **Random audits** | Probabilistic quality checks with penalties for low-quality interactions |\n",
    "| 7 | **Circuit breaker** | Freezes agents that exceed toxicity thresholds |\n",
    "| 8 | **Transaction tax** | Per-interaction tax on surplus, redistributed to ecosystem |\n",
    "| 9 | **Staking / bonding** | Minimum stake to participate; stake slashed for bad behavior |\n",
    "| 10 | **Moderator agent** | Continuous oversight with moderate penalties for flagged interactions |\n",
    "| 11 | **Collusion detection** | Detects and penalizes coordinated adversarial pairs |\n",
    "| 12 | **Combined defense-in-depth** | Multiple mechanisms layered together |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from swarm.scenarios import load_scenario\n",
    "\n",
    "# Add examples/ to path so we can import the sweep runner\n",
    "examples_dir = Path(\"examples\") if Path(\"examples\").is_dir() else Path(\"../examples\")\n",
    "sys.path.insert(0, str(examples_dir))\n",
    "from governance_mvp_sweep import run_governance_sweep, print_summary\n",
    "\n",
    "# Locate scenario files\n",
    "SCENARIOS_DIR = Path(\"scenarios\") if Path(\"scenarios\").is_dir() else Path(\"../scenarios\")\n",
    "print(\"Scenarios directory:\", SCENARIOS_DIR.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load base scenario and run the 12-configuration sweep.\n",
    "# Using n_epochs=5 for faster execution in Colab; increase for more stable results.\n",
    "base_scenario = load_scenario(SCENARIOS_DIR / \"baseline.yaml\")\n",
    "\n",
    "results = run_governance_sweep(\n",
    "    base_scenario,\n",
    "    runs_per_config=1,\n",
    "    seed_base=42,\n",
    "    n_epochs=5,\n",
    ")\n",
    "print(f\"\\nCompleted {len(results)} runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the formatted summary table\n",
    "print_summary(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to a DataFrame for plotting\n",
    "rows = []\n",
    "for r in results:\n",
    "    sr = r.sweep_result\n",
    "    rows.append({\n",
    "        \"mechanism\": r.label,\n",
    "        \"welfare\": sr.total_welfare,\n",
    "        \"toxicity\": sr.avg_toxicity,\n",
    "        \"quality_gap\": sr.avg_quality_gap,\n",
    "        \"n_frozen\": sr.n_frozen,\n",
    "        \"honest_payoff\": sr.honest_avg_payoff,\n",
    "        \"opportunistic_payoff\": sr.opportunistic_avg_payoff,\n",
    "        \"deceptive_payoff\": sr.deceptive_avg_payoff,\n",
    "        \"adversarial_payoff\": sr.adversarial_avg_payoff,\n",
    "        \"avg_reputation\": sr.avg_reputation,\n",
    "        \"acceptance_rate\": sr.accepted_interactions / max(sr.total_interactions, 1),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle(\"Governance MVP Sweep: 12 Mechanism Comparison\", fontsize=15, fontweight=\"bold\")\n",
    "\n",
    "# --- 1. Welfare by mechanism (horizontal bar) ---\n",
    "ax = axes[0, 0]\n",
    "colors = [\"#d62728\" if m == \"baseline\" else \"#2ca02c\" if m == \"combined_defense_in_depth\" else \"#1f77b4\"\n",
    "          for m in df[\"mechanism\"]]\n",
    "ax.barh(df[\"mechanism\"], df[\"welfare\"], color=colors)\n",
    "ax.set_xlabel(\"Total Welfare\")\n",
    "ax.set_title(\"Welfare by Mechanism\")\n",
    "ax.invert_yaxis()\n",
    "ax.axvline(x=df.loc[df[\"mechanism\"] == \"baseline\", \"welfare\"].values[0],\n",
    "           color=\"#d62728\", linestyle=\"--\", alpha=0.6, label=\"Baseline\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- 2. Toxicity comparison ---\n",
    "ax = axes[0, 1]\n",
    "ax.barh(df[\"mechanism\"], df[\"toxicity\"], color=[\"#d62728\" if t > df.loc[0, \"toxicity\"] else \"#2ca02c\"\n",
    "        for t in df[\"toxicity\"]])\n",
    "ax.set_xlabel(\"Average Toxicity Rate\")\n",
    "ax.set_title(\"Toxicity by Mechanism\")\n",
    "ax.invert_yaxis()\n",
    "ax.axvline(x=df.loc[df[\"mechanism\"] == \"baseline\", \"toxicity\"].values[0],\n",
    "           color=\"#d62728\", linestyle=\"--\", alpha=0.6, label=\"Baseline\")\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- 3. Honest vs Adversarial payoff ratio ---\n",
    "ax = axes[1, 0]\n",
    "# Combine deceptive + adversarial as \"adversarial\" payoff for comparison\n",
    "adv_payoff = df[[\"deceptive_payoff\", \"adversarial_payoff\"]].max(axis=1)\n",
    "honest_payoff = df[\"honest_payoff\"]\n",
    "# Avoid division by zero\n",
    "ratio = honest_payoff / adv_payoff.replace(0, np.nan)\n",
    "ratio = ratio.fillna(0)\n",
    "bar_colors = [\"#2ca02c\" if r > 1 else \"#d62728\" for r in ratio]\n",
    "ax.barh(df[\"mechanism\"], ratio, color=bar_colors)\n",
    "ax.set_xlabel(\"Honest / Adversarial Payoff Ratio\")\n",
    "ax.set_title(\"Honest vs Adversarial Advantage\")\n",
    "ax.axvline(x=1.0, color=\"gray\", linestyle=\"--\", alpha=0.6, label=\"Parity\")\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=9)\n",
    "\n",
    "# --- 4. Per-type payoff grouped bar ---\n",
    "ax = axes[1, 1]\n",
    "x = np.arange(len(df))\n",
    "width = 0.2\n",
    "ax.barh(x - 1.5*width, df[\"honest_payoff\"], width, label=\"Honest\", color=\"#2ca02c\")\n",
    "ax.barh(x - 0.5*width, df[\"opportunistic_payoff\"], width, label=\"Opportunistic\", color=\"#ff7f0e\")\n",
    "ax.barh(x + 0.5*width, df[\"deceptive_payoff\"], width, label=\"Deceptive\", color=\"#d62728\")\n",
    "ax.barh(x + 1.5*width, df[\"adversarial_payoff\"], width, label=\"Adversarial\", color=\"#9467bd\")\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(df[\"mechanism\"])\n",
    "ax.set_xlabel(\"Average Payoff\")\n",
    "ax.set_title(\"Per-Type Payoff by Mechanism\")\n",
    "ax.invert_yaxis()\n",
    "ax.legend(fontsize=8, loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**What to look for in the plots above:**\n",
    "\n",
    "- **Welfare (top-left):** Which mechanisms increase total ecosystem welfare above the baseline? The combined defense-in-depth (green) typically outperforms any single mechanism.\n",
    "- **Toxicity (top-right):** Lower is better. Mechanisms like circuit breakers and random audits directly target toxic interactions. Green bars indicate improvement over the baseline.\n",
    "- **Honest vs Adversarial advantage (bottom-left):** A ratio above 1.0 (green) means honest agents earn more than adversarial ones -- the system rewards good behavior. Ratios below 1.0 (red) indicate the mechanism fails to deter exploitation.\n",
    "- **Per-type payoffs (bottom-right):** The ideal governance mechanism increases honest payoffs while decreasing deceptive/adversarial payoffs.\n",
    "\n",
    "### General patterns\n",
    "\n",
    "1. **No single mechanism is sufficient.** Each lever addresses a different attack vector -- reputation gaming, bandwidth flooding, collusion, etc.\n",
    "2. **Defense-in-depth works best.** Combining multiple mechanisms provides broader coverage and makes it harder for adversaries to find a single exploit path.\n",
    "3. **Some mechanisms have trade-offs.** Transaction taxes reduce welfare for everyone (including honest agents). Circuit breakers can over-freeze in edge cases. Tuning matters.\n",
    "4. **Collusion detection is a critical differentiator.** In scenarios with coordinated adversaries, collusion detection is often the mechanism that prevents ecosystem collapse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "- **Increase `runs_per_config`** to 3-5 for more statistically robust results (averages over multiple seeds).\n",
    "- **Increase `n_epochs`** to 10-20 for longer-horizon dynamics (some mechanisms only show effects after several epochs).\n",
    "- **Try different base scenarios** -- run the sweep against `adversarial_redteam.yaml` to test mechanisms under stress.\n",
    "- **Export results to CSV** for further analysis:\n",
    "\n",
    "```python\n",
    "from governance_mvp_sweep import export_csv\n",
    "export_csv(results, Path(\"governance_sweep_results.csv\"))\n",
    "```\n",
    "\n",
    "- **Run the full sweep from the CLI** with more seeds:\n",
    "\n",
    "```bash\n",
    "python examples/governance_mvp_sweep.py --runs-per-config 5 --epochs 10\n",
    "```\n",
    "\n",
    "**Repository:** [github.com/swarm-ai-safety/swarm](https://github.com/swarm-ai-safety/swarm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}